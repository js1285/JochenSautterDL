{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sixth exercise (Chapter 8)\n",
    "\n",
    "In this exercise we consider Chapter 8 of the book \"Deep Learning\". The exercise focuses on some optimization techniques  that represent the state-of-the-art for training neural networks.\n",
    "In particular, you will implement SGD with different learning rate schedules, SGD with momentum and ADAM algorithms, and use them to train a small feedforward neural network on the MNIST dataset. Fianlly, you will be asked to answer few questions about second order methods.\n",
    "\n",
    "List of covered topics: \n",
    "\n",
    "* Stochastic Gradient Descent with different learning rate decay heuristics (8.3.1)\n",
    "* SGD with Momentum (8.3.2)\n",
    "* Adam (8.5.3)\n",
    "* Newton's Method (8.6.1)\n",
    "\n",
    "Apart from the MNIST dataset and the mnist_loader.py file, both available on ILIAS, we also need a Python library called Numpy, for doing fast linear algebra. If you don't already have Numpy installed, please install it. \n",
    "\n",
    "### NOTE\n",
    "In order to speed up the training, reduce the training set size to the first 10000 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "The following code implements a feedforward neural network together with the backpropagation and SGD algorithms. In the next points of this exercise you will be asked to add some extra optimization features to this implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "\n",
    "# Define the Quadratic and the Cross-Entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "        \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    "\n",
    "# Learning Rate Schedule\n",
    "\n",
    "class Constant(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def lr(lr_0, stepsize, nepochs):\n",
    "        '''lr_0: initial value of the learning rate\n",
    "           stepsize: length of step size (to be used only with CosineDecayRestart and Step classes)\n",
    "           nepochs: total number of epochs\n",
    "           \n",
    "           the function should return a list of length=nepochs where element i is the value of the learning rate\n",
    "           at epoch i \n",
    "        '''\n",
    "        return [lr_0] * nepochs\n",
    "\n",
    "class Step(object):\n",
    "    \n",
    "    def lr(lr_0, stepsize, nepochs):\n",
    "        '''lr_0: initial value of the learning rate\n",
    "           stepsize: length of step size (to be used only with CosineDecayRestart and Step classes)\n",
    "           nepochs: total number of epochs\n",
    "           \n",
    "           the function should return a list of length=nepochs where element i is the value of the learning rate\n",
    "           at epoch i \n",
    "        '''\n",
    "        #TODO:learning_rate\n",
    "        raise NotImplementedError \n",
    "\n",
    "class Linear(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def lr(lr_0, stepsize, nepochs):\n",
    "        '''lr_0: initial value of the learning rate\n",
    "           stepsize: length of step size (to be used only with CosineDecayRestart and Step classes)\n",
    "           nepochs: total number of epochs\n",
    "           \n",
    "           the function should return a list of length=nepochs where element i is the value of the learning rate\n",
    "           at epoch i \n",
    "        '''\n",
    "        #TODO:learning_rate\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class CosineDecay(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def lr(lr_0, stepsize, nepochs):\n",
    "        '''lr_0: initial value of the learning rate\n",
    "           stepsize: length of step size (to be used only with CosineDecayRestart and Step classes)\n",
    "           nepochs: total number of epochs\n",
    "           \n",
    "           the function should return a list of length=nepochs where element i is the value of the learning rate\n",
    "           at epoch i \n",
    "        '''\n",
    "        #TODO:learning_rate\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class CosineDecayRestart(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def lr(lr_0, stepsize, nepochs):\n",
    "        '''lr_0: initial value of the learning rate\n",
    "           stepsize: length of step size (to be used only with CosineDecayRestart and Step classes)\n",
    "           nepochs: total number of epochs\n",
    "           \n",
    "           the function should return a list of length=nepochs where element i is the value of the learning rate\n",
    "           at epoch i \n",
    "        '''\n",
    "        #TODO:learning_rate\n",
    "        raise NotImplementedError \n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=QuadraticCost, lr_schedule=Constant):\n",
    "\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.cost = cost\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.velocity_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.velocity_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, stepsize=1, lr_0=0.1,\n",
    "            momentum=False,\n",
    "            alpha=0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False,\n",
    "            filename_config=None,\n",
    "            filename_results=None):\n",
    "      \n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        #TODO:learning_rate\n",
    "        '''NOTE: lr_list is a list containing the learning rate values to be used for the training'''\n",
    "        lr_list = self.lr_schedule.lr(lr_0=lr_0, stepsize=stepsize, nepochs=epochs)\n",
    "        if not lr_list:\n",
    "            raise NotImplementedError \n",
    "         \n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, lr_list[j], momentum , alpha)\n",
    "                \n",
    "            print (\"Epoch {0} complete\".format(j))\n",
    "            \n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data)\n",
    "                training_cost.append(cost)\n",
    "                print (\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data)\n",
    "                evaluation_cost.append(cost)\n",
    "                print (\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "        return training_cost, training_accuracy, evaluation_cost, evaluation_accuracy, lr_list\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, momentum, alpha):\n",
    "    \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "            \n",
    "        if momentum:\n",
    "            self.velocity_b = [alpha * v - (eta/len(mini_batch)) * d_b for (v, d_b) in zip(self.velocity_b, nabla_b)]\n",
    "            self.velocity_w = [alpha * v - (eta/len(mini_batch)) * d_w for (v, d_w) in zip(self.velocity_w, nabla_w)]\n",
    "            self.biases =  [b + v for (b, v) in zip(self.biases, self.velocity_b)]\n",
    "            self.weights = [w + v for (w, v) in zip(self.weights, self.velocity_w)]\n",
    "            \n",
    "            #TODO:momentum\n",
    "            #raise NotImplementedError \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] \n",
    "    \n",
    "\n",
    "    def Adam(self, training_data, epochs, mini_batch_size, stepsize=1, lr_0=0.1,\n",
    "            rho_1=0.9,\n",
    "            rho_2=0.999,\n",
    "            delta=10**(-8),\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False,\n",
    "            filename_config=None,\n",
    "            filename_results=None):\n",
    "      \n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        lr_list = (self.lr_schedule).lr(lr_0, stepsize, epochs)\n",
    "        t = 0\n",
    "        \n",
    "        # initialize moment estimates\n",
    "        s_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        s_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        r_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        r_w = [np.zeros(w.shape) for w in self.weights]  \n",
    "        self.first_moment_estimates = (s_b, s_w)\n",
    "        self.second_moment_estimates = (r_b, r_w)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                t = t+1\n",
    "                self.update_mini_batch_adam(mini_batch, lr_list[j], rho_1, rho_2, delta, t)\n",
    "                \n",
    "            print (\"Epoch {0} complete\".format(j))\n",
    "            \n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data)\n",
    "                training_cost.append(cost)\n",
    "                print (\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data)\n",
    "                evaluation_cost.append(cost)\n",
    "                print (\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "        return training_cost, training_accuracy, evaluation_cost, evaluation_accuracy, lr_list\n",
    "\n",
    "    def update_mini_batch_adam(self, mini_batch, eta, rho_1, rho_2, delta, t):\n",
    "        '''\n",
    "        mini_batch: the mini batch considered in the current subepoch \n",
    "        eta: value of the learning rate for the current epoch\n",
    "        rho_1: Adam's parameter (see par. 8.5.3)\n",
    "        rho_2: Adam's parameter (see par. 8.5.3)\n",
    "        delta: Adam's correction term (see par. 8.5.3)\n",
    "        t: number of gradient's updates that has been done so far\n",
    "        \n",
    "        the function should update the parameters of the network (self.weights and self.biases) \n",
    "        based on Adam's update rule.\n",
    "        '''\n",
    "        # compute gradient as usually\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # unpack moment estimates\n",
    "        (s_b, s_w) = self.first_moment_estimates\n",
    "        (r_b, r_w) = self.second_moment_estimates\n",
    "            \n",
    "        # first moment estimate\n",
    "        s_b = [rho_1 * s + (1 - rho_1) * d for (s, d) in zip(s_b, nabla_b)]\n",
    "        s_w = [rho_1 * s + (1 - rho_1) * d for (s, d) in zip(s_w, nabla_w)]\n",
    "        \n",
    "        # second moment estimate\n",
    "        r_b = [rho_2 * r + (1 - rho_2) * d * d for (r, d) in zip(r_b, nabla_b)]\n",
    "        r_w = [rho_2 * r + (1 - rho_2) * d * d for (r, d) in zip(r_w, nabla_w)]\n",
    "        # (in numpy, ndarray multiplication is by default element-wise)\n",
    "        \n",
    "        # correct bias in first moment\n",
    "        s_hat_b = [s / (1 - rho_1 ** t) for s in s_b]\n",
    "        s_hat_w = [s / (1 - rho_1 ** t) for s in s_w]\n",
    "        \n",
    "        # correct bias in second moment\n",
    "        r_hat_b = [r / (1 - rho_2 ** t) for r in r_b]\n",
    "        r_hat_w = [r / (1 - rho_2 ** t) for r in r_w]\n",
    "        \n",
    "        # compute and apply update\n",
    "        self.biases = [b - eta * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                       for (b, s_hat, r_hat) in zip(self.biases, s_hat_b, r_hat_b)]\n",
    "        self.weights = [w - eta * s_hat / (np.sqrt(r_hat) + delta)\n",
    "                        for (w, s_hat, r_hat) in zip(self.weights, s_hat_w, r_hat_w)]\n",
    "        # again, all operations element-wise\n",
    "        \n",
    "        # save moment estimates\n",
    "        self.first_moment_estimates = (s_b, s_w)\n",
    "        self.second_moment_estimates = (r_b, r_w)\n",
    "        \n",
    "        #TODO:Adam\n",
    "        # raise NotImplementedError \n",
    "            \n",
    "    def backprop(self, x, y):\n",
    "  \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used in the following way:\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def accuracy(self, data):\n",
    "\n",
    "        results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data):\n",
    "    \n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD) with different learning rate decay heuristics\n",
    "\n",
    "Stochastic gradient descent based algorithms represent the-state-of-the-art to train deep neural networks. As you might have experienced, and as it is stated in the book, one of the major drawbacks of these methods is their great sensitivity to the learning rate value: performances indeed change dramatically with different values of this parameter. In exercise 04 we trained an MLP with SGD using a fixed learning rate value:in general practitioners decrease it to have better performances.Implement the following different heuristics for the learning rate value: constant, step decay, linear decay, cosine decay, and cosine decay with restarts. In order to achieve this goal, follow the '#TODO:learning_rate' indications in the code. Then use each one of these heuristics to train a feedforward neural network with 2 hidden layers of 30 and 100 neurons respectively. In particular: set the initial value for the learning rate to 3.0, the stepsize to 7, and train the network for 30 epochs with a batch size of 10 on the MNIST dataset. Finally, generate two plots for each experiment: learning rate vs epochs, and test accuracy vs epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "\n",
    "training_data, validation_data, test_data = mnist_loader.load_datawrapper()\n",
    "training_data = training_data[:10000]\n",
    "\n",
    "\n",
    "num_training_examples = len(training_data) \n",
    "num_validation_examples = len(validation_data)\n",
    "num_test_examples = len(test_data)\n",
    "\n",
    "num_features = len(training_data[0][0])\n",
    "num_classes = len(training_data[0][1])\n",
    "\n",
    "#TODO:learning_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:learning_rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO:learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#TODO:learning_rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "In presence of an ill conditioned Hessian matrix, it might be useful to consider an exponentially decaying moving average of the past gradients in order to speed up the learning. SGD with momentum (see equations 8.15 and 8.16 in the book) has indeed been designed to take into consideration not only the current direction given by the negative gradient, but also how large and aligned is a sequence of past gradients with respect to the current one. Modify your previous implementation of SGD to incorporate the momentum by following the '#TODO:momentum' instructions in the code, then use it to train the network on the MNIST dataset with an initial learning rate of 3.0, cosine with restarts decay and $\\alpha= 0.3$. Finally plot the results obtained: test accuracy vs number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n",
      "Cost on training data: 0.849862470032989\n",
      "Accuracy on training data: 8538 / 10000\n",
      "Cost on evaluation data: 238.58055687888552\n",
      "Accuracy on evaluation data: 1035 / 10000\n",
      "Epoch 1 complete\n",
      "Cost on training data: 0.6829957741955803\n",
      "Accuracy on training data: 8863 / 10000\n",
      "Cost on evaluation data: 268.1380994850152\n",
      "Accuracy on evaluation data: 907 / 10000\n",
      "Epoch 2 complete\n",
      "Cost on training data: 0.4899691138947409\n",
      "Accuracy on training data: 9194 / 10000\n",
      "Cost on evaluation data: 269.9424761804021\n",
      "Accuracy on evaluation data: 1015 / 10000\n",
      "Epoch 3 complete\n",
      "Cost on training data: 0.42236762981001463\n",
      "Accuracy on training data: 9318 / 10000\n",
      "Cost on evaluation data: 282.238143042809\n",
      "Accuracy on evaluation data: 1062 / 10000\n",
      "Epoch 4 complete\n",
      "Cost on training data: 0.3770250980139128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-22f4ce8fb07b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                   \u001b[0mmonitor_evaluation_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                   \u001b[0mmonitor_training_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                   monitor_training_accuracy=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-aae45bc2b4af>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, stepsize, lr_0, momentum, alpha, evaluation_data, monitor_evaluation_cost, monitor_evaluation_accuracy, monitor_training_cost, monitor_training_accuracy, filename_config, filename_results)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Cost on training data: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmonitor_training_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mtraining_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 print (\"Accuracy on training data: {} / {}\".format(\n",
      "\u001b[0;32m<ipython-input-36-aae45bc2b4af>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-aae45bc2b4af>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-aae45bc2b4af>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO:momentum\n",
    "net = Network(sizes=[784, 30, 100, 10], cost=CrossEntropyCost)\n",
    "results = net.SGD(training_data=training_data, epochs=30, mini_batch_size=50, lr_0=3.0,\n",
    "                  momentum=True, alpha=0.3,\n",
    "                  evaluation_data=validation_data,\n",
    "                  monitor_evaluation_cost=True,\n",
    "                  monitor_evaluation_accuracy=True,\n",
    "                  monitor_training_cost=True,\n",
    "                  monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mention another method that could be used to address the ill conditioning of the Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conjugate gradients? In regions of high curvature, CG may be better at finding the directions where the function does not grow again.\n",
    "- Adaptive gradient methods may reduce step size to a values small enough so that linear terms dominate quadratic terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Adam is one of the most popular algorithms used for training neural networks. Implement it in the code where required by the '#TODO:Adam' instructions, and use it train our network on the MNSIT dataset. In particular, set $\\rho_1 = 0.9$, $\\rho_2 = 0.999$ and $\\delta=10^{-8}$. Plot the results obtained (test accuracy vs number of epochs) for a fixed learning rate of 0.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n",
      "Cost on training data: 1.0870747871251307\n",
      "Accuracy on training data: 8199 / 10000\n",
      "Cost on evaluation data: 252.09141136370505\n",
      "Accuracy on evaluation data: 1121 / 10000\n",
      "Epoch 1 complete\n",
      "Cost on training data: 0.9388306308460481\n",
      "Accuracy on training data: 8425 / 10000\n",
      "Cost on evaluation data: 247.71781520138956\n",
      "Accuracy on evaluation data: 1096 / 10000\n",
      "Epoch 2 complete\n",
      "Cost on training data: 0.8363399678945608\n",
      "Accuracy on training data: 8679 / 10000\n",
      "Cost on evaluation data: 253.3564116207289\n",
      "Accuracy on evaluation data: 885 / 10000\n",
      "Epoch 3 complete\n",
      "Cost on training data: 1.0083424246501675\n",
      "Accuracy on training data: 8386 / 10000\n",
      "Cost on evaluation data: 280.63378391733073\n",
      "Accuracy on evaluation data: 897 / 10000\n",
      "Epoch 4 complete\n",
      "Cost on training data: 0.9091695616815991\n",
      "Accuracy on training data: 8624 / 10000\n",
      "Cost on evaluation data: 276.3232809004797\n",
      "Accuracy on evaluation data: 1085 / 10000\n",
      "Epoch 5 complete\n",
      "Cost on training data: 0.9051493602956914\n",
      "Accuracy on training data: 8503 / 10000\n",
      "Cost on evaluation data: 272.7692691428024\n",
      "Accuracy on evaluation data: 937 / 10000\n",
      "Epoch 6 complete\n",
      "Cost on training data: 0.8498620488199985\n",
      "Accuracy on training data: 8639 / 10000\n",
      "Cost on evaluation data: 259.72523750369777\n",
      "Accuracy on evaluation data: 958 / 10000\n",
      "Epoch 7 complete\n",
      "Cost on training data: 0.794423899128083\n",
      "Accuracy on training data: 8830 / 10000\n",
      "Cost on evaluation data: 244.65784305907627\n",
      "Accuracy on evaluation data: 1048 / 10000\n",
      "Epoch 8 complete\n",
      "Cost on training data: 0.7396957495767775\n",
      "Accuracy on training data: 8840 / 10000\n",
      "Cost on evaluation data: 290.4727287922807\n",
      "Accuracy on evaluation data: 1026 / 10000\n",
      "Epoch 9 complete\n",
      "Cost on training data: 0.7818951701396487\n",
      "Accuracy on training data: 8766 / 10000\n",
      "Cost on evaluation data: 298.7867794153969\n",
      "Accuracy on evaluation data: 1022 / 10000\n",
      "Epoch 10 complete\n",
      "Cost on training data: 0.7623837953951857\n",
      "Accuracy on training data: 8839 / 10000\n",
      "Cost on evaluation data: 270.2574558489207\n",
      "Accuracy on evaluation data: 952 / 10000\n",
      "Epoch 11 complete\n",
      "Cost on training data: 0.6844710758995676\n",
      "Accuracy on training data: 8939 / 10000\n",
      "Cost on evaluation data: 290.00059796505394\n",
      "Accuracy on evaluation data: 980 / 10000\n",
      "Epoch 12 complete\n",
      "Cost on training data: 0.7341620065137339\n",
      "Accuracy on training data: 8777 / 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on evaluation data: 280.20839008816455\n",
      "Accuracy on evaluation data: 960 / 10000\n",
      "Epoch 13 complete\n",
      "Cost on training data: 0.7939139084736349\n",
      "Accuracy on training data: 8724 / 10000\n",
      "Cost on evaluation data: 293.471720924871\n",
      "Accuracy on evaluation data: 850 / 10000\n",
      "Epoch 14 complete\n",
      "Cost on training data: 0.7455422940490054\n",
      "Accuracy on training data: 8812 / 10000\n",
      "Cost on evaluation data: 289.1417853153425\n",
      "Accuracy on evaluation data: 1032 / 10000\n",
      "Epoch 15 complete\n",
      "Cost on training data: 0.6804208299408319\n",
      "Accuracy on training data: 8996 / 10000\n",
      "Cost on evaluation data: 299.8709076212503\n",
      "Accuracy on evaluation data: 1101 / 10000\n",
      "Epoch 16 complete\n",
      "Cost on training data: 0.6520915273240159\n",
      "Accuracy on training data: 8904 / 10000\n",
      "Cost on evaluation data: 299.35108265643845\n",
      "Accuracy on evaluation data: 1009 / 10000\n",
      "Epoch 17 complete\n",
      "Cost on training data: 0.7467924964230159\n",
      "Accuracy on training data: 8908 / 10000\n",
      "Cost on evaluation data: 289.7310288870654\n",
      "Accuracy on evaluation data: 1019 / 10000\n",
      "Epoch 18 complete\n",
      "Cost on training data: 0.8015321217528747\n",
      "Accuracy on training data: 8708 / 10000\n",
      "Cost on evaluation data: 271.51295366690766\n",
      "Accuracy on evaluation data: 1074 / 10000\n",
      "Epoch 19 complete\n",
      "Cost on training data: 0.7520681176373138\n",
      "Accuracy on training data: 8829 / 10000\n",
      "Cost on evaluation data: 300.692137854809\n",
      "Accuracy on evaluation data: 1020 / 10000\n",
      "Epoch 20 complete\n",
      "Cost on training data: 0.6770364659599157\n",
      "Accuracy on training data: 8868 / 10000\n",
      "Cost on evaluation data: 278.8156168854923\n",
      "Accuracy on evaluation data: 1001 / 10000\n",
      "Epoch 21 complete\n",
      "Cost on training data: 0.8031787197875749\n",
      "Accuracy on training data: 8808 / 10000\n",
      "Cost on evaluation data: 315.5981004958114\n",
      "Accuracy on evaluation data: 928 / 10000\n",
      "Epoch 22 complete\n",
      "Cost on training data: 0.7399929899207173\n",
      "Accuracy on training data: 8913 / 10000\n",
      "Cost on evaluation data: 327.1596374199376\n",
      "Accuracy on evaluation data: 938 / 10000\n",
      "Epoch 23 complete\n",
      "Cost on training data: 0.6815108504411543\n",
      "Accuracy on training data: 8922 / 10000\n",
      "Cost on evaluation data: 291.5386008032416\n",
      "Accuracy on evaluation data: 1130 / 10000\n",
      "Epoch 24 complete\n",
      "Cost on training data: 0.6839827580339174\n",
      "Accuracy on training data: 8835 / 10000\n",
      "Cost on evaluation data: 288.5330248000676\n",
      "Accuracy on evaluation data: 1031 / 10000\n",
      "Epoch 25 complete\n",
      "Cost on training data: 0.6355662344139701\n",
      "Accuracy on training data: 8936 / 10000\n",
      "Cost on evaluation data: 311.2762865201923\n",
      "Accuracy on evaluation data: 1171 / 10000\n",
      "Epoch 26 complete\n",
      "Cost on training data: 0.6392299075652074\n",
      "Accuracy on training data: 8994 / 10000\n",
      "Cost on evaluation data: 279.8891864860716\n",
      "Accuracy on evaluation data: 1038 / 10000\n",
      "Epoch 27 complete\n",
      "Cost on training data: 0.6457454812485866\n",
      "Accuracy on training data: 8979 / 10000\n",
      "Cost on evaluation data: 292.04518515451014\n",
      "Accuracy on evaluation data: 956 / 10000\n",
      "Epoch 28 complete\n",
      "Cost on training data: 0.63041473511418\n",
      "Accuracy on training data: 9088 / 10000\n",
      "Cost on evaluation data: 288.2033037349076\n",
      "Accuracy on evaluation data: 999 / 10000\n",
      "Epoch 29 complete\n",
      "Cost on training data: 0.579473536767284\n",
      "Accuracy on training data: 9147 / 10000\n",
      "Cost on evaluation data: 318.6214147863107\n",
      "Accuracy on evaluation data: 1014 / 10000\n"
     ]
    }
   ],
   "source": [
    "#TODO:Adam\n",
    "net = Network(sizes=[784, 30, 100, 10], cost=CrossEntropyCost)\n",
    "results = net.Adam(training_data=training_data, epochs=30, mini_batch_size=50, lr_0=0.1,\n",
    "                   evaluation_data=validation_data,\n",
    "                   monitor_evaluation_cost=True,\n",
    "                   monitor_evaluation_accuracy=True,\n",
    "                   monitor_training_cost=True,\n",
    "                   monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly explain the Newton's method and why it is not possible to use it for training networks with a significant number of parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
