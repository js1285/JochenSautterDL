{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import os\n",
    "import gzip\n",
    "from IPython.core.debugger import set_trace\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First exercise: Classifying MNIST with MLPs\n",
    "In this exercise you will implement a Neural Network (or MLP) and classify the MNIST digits with it.\n",
    "MNIST is a \"well hung\" dataset that has been used a lot over the years to benchmark different classification algorithms. \n",
    "To learn more about it have a look here: http://yann.lecun.com/exdb/mnist/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "We first define a function for downloading and loading MNIST.\n",
    "**WARNING**: Executing it will obviously use up some space on your machine ;). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world further test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist(datasets_dir='./data'):\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        os.mkdir(datasets_dir)\n",
    "    data_file = os.path.join(datasets_dir, 'mnist.pkl.gz')\n",
    "    if not os.path.exists(data_file):\n",
    "        print('... downloading MNIST from the web')\n",
    "        try:\n",
    "            import urllib\n",
    "            urllib.urlretrieve('http://google.com')\n",
    "        except AttributeError:\n",
    "            import urllib.request as urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        urllib.urlretrieve(url, data_file)\n",
    "\n",
    "    print('... loading data')\n",
    "    # Load the dataset\n",
    "    f = gzip.open(data_file, 'rb')\n",
    "    try:\n",
    "        train_set, valid_set, test_set = cPickle.load(f, encoding=\"latin1\")\n",
    "    except TypeError:\n",
    "        train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    test_x, test_y = test_set\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x = test_x.astype('float32').reshape(test_x.shape[0], 1, 28, 28)\n",
    "    test_y = test_y.astype('int32')\n",
    "    valid_x, valid_y = valid_set\n",
    "    valid_x = valid_x.astype('float32')\n",
    "    valid_x = valid_x.astype('float32').reshape(valid_x.shape[0], 1, 28, 28)\n",
    "    valid_y = valid_y.astype('int32')\n",
    "    train_x, train_y = train_set\n",
    "    train_x = train_x.astype('float32').reshape(train_x.shape[0], 1, 28, 28)\n",
    "    train_y = train_y.astype('int32')\n",
    "    rval = [(train_x, train_y), (valid_x, valid_y), (test_x, test_y)]\n",
    "    print('... done loading data')\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Layers\n",
    "We now define \"bare bone\" neural network layers.\n",
    "The parts marked with **TODO** are where you should finish the implementation!\n",
    "Conceptually we will implement the layers as follows:\n",
    "\n",
    "Each layer has a constructor (\"__init__\") that takes an input layer plus some additional arguments such as layer size and the activation function name. The layer then uses the provided input layer to compute the layer dimensions, weight shapes, etc. and setup all auxilliary variables.\n",
    "\n",
    "Each layer then has to provide three functions (as defined in the Layer class below): *output_shape()*, *fprop()* and *brop()*. The output_shape function is used to figure out the shape for the next layer and the *fprop()/bprop()* functions are used to compute forward and backward passes through the network.\n",
    "\n",
    "MY comments:\n",
    "Weights and gradients w,b,dw,db, are Parameters of Layers classes. \n",
    "The forward Data flow, Activations (A1, Z1...) are not stored in the classes, but rather flowing through them, as \"input\" for the fprop() methods. But \"last_input\" is stored, in order to compute the gradients.\n",
    "Model is a layer z1,a1,w1,b1,h1() with input layer z0,... and output layer z2,....\n",
    "\n",
    "Dimensions: n = n_samples, D1, D0, D2 nodes in current, previous, following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by defining simple helpers\n",
    "def sigmoid(x):     # sigmoid to be used for forward pass\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_d(x):   # differential of sigmoid to be used for backward pass (method bprop)\n",
    "    dx = np.zeros_like(x)  # initialise matrix\n",
    "    dx = sigmoid(x) * (1.0 - sigmoid(x))    # 1.0+ is broadcasted,  \" * \"   is element-wise\n",
    "    return dx\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_d(x):\n",
    "    dx = np.zeros_like(x)  # initialise matrix\n",
    "    dx = 1 - (np.tanh(x))**2  # tanh and sqr elementwise, \"1 -\" is broadcasted\n",
    "    return dx        \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def relu_d(x):\n",
    "    dx = np.zeros_like(x)  # initialise matrix\n",
    "    dx[x > 0] = 1.         # python stylish elementwise boolean indexing\n",
    "    return dx     \n",
    "\n",
    "def softmax(x, axis=1):   # axis = 1: summmiert die Spalten in eine Spalte zusammen (siehe 00-Mystuff)\n",
    "    # input: output of last layer (=a3, as last layer has no linearity) --> array (n,10), one colon / class/digit\n",
    "    # softmax applied on each row soft_k e^ak / sum(e^ai) squeezes each row into figures <1, with strong emphasis \n",
    "    # on the largest one. Cols sum up to 1 --> can be interpreted as probability distribution among classes \n",
    "    # to make the softmax a \"safe\" operation we will \n",
    "    # first subtract the maximum along the specified axis\n",
    "    # so that np.exp(x) does not blow up!\n",
    "    # Note that this does not change the output.  # kürzt sich raus \n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    x_safe = x - x_max   \n",
    "    e_x = np.exp(x_safe)\n",
    "    # result has same shape as x, but transformed to values between 0 and 1, in each row / sample strong emphasis on largest value\n",
    "    # np.sum summiert in jeder Zeile die Spalten zusammen (Sum over features)\n",
    "    return e_x / np.sum(e_x, axis=axis, keepdims=True)\n",
    "\n",
    "# \"softmax_d\" is not used (only used for output layer, and then gradient together with loss function)\n",
    "\n",
    "def one_hot(labels):\n",
    "    \"\"\"this creates a one hot encoding from a flat vector:\n",
    "    i.e. given y = [0,2,1]\n",
    "     it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)   # how many different elements ([0,3,2,3,2] --> 3)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))  # concatenate tuples (3,) and (3,) --> array (3,3) see mystuff\n",
    "    # set_trace()\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "def unhot(one_hot_labels):\n",
    "    \"\"\" Invert a one hot encoding, creating a flat vector \"\"\"\n",
    "    # argmax Returns the indices of the maximum values along an axis.\n",
    "    return np.argmax(one_hot_labels, axis=-1)   # axis=-1 is last dimension . e.g. =1 if two dim array \n",
    "\n",
    "# then define an activation function class, is instantiated when calling (__init__) FullyConnectedLayer\n",
    "class Activation(object):\n",
    "    \n",
    "    def __init__(self, tname):\n",
    "        if tname == 'sigmoid':\n",
    "            self.act = sigmoid       # assign appropriate function \n",
    "            self.act_d = sigmoid_d\n",
    "        elif tname == 'tanh':\n",
    "            self.act = tanh\n",
    "            self.act_d = tanh_d\n",
    "        elif tname == 'relu':\n",
    "            self.act = relu\n",
    "            self.act_d = relu_d\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function.')\n",
    "                                     # fprop called as subroutine from within layer.fprop()\n",
    "    def fprop(self, input):          # apply selected activation function on linear input: z1 = h1(a1)\n",
    "        self.last_input = input      # store a1 for later use in bprop\n",
    "        return self.act(input)       # h1(a1) call sigmoid, tanh or relu and apply on input \n",
    "    \n",
    "    def bprop(self, output_grad):    # output_grad is dL/dz1, passed by calling method (layer)  (n,D1)\n",
    "        # set_trace()\n",
    "        return output_grad * self.act_d(self.last_input)   # dL/da1 = dL/dz1 * h1'(a1) \"*\" denoting hadamard product\n",
    "\n",
    "# define a base class for layers - purpose? --> if it does nothing, and everything has to be defined in derived classes ? \n",
    "class Layer(object):\n",
    "    \n",
    "    def fprop(self, input):   \n",
    "        \"\"\" Calculate layer output for given input \n",
    "            (forward propagation). \n",
    "        \"\"\"\n",
    "        # has to be overloaded by derived class\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient and gradient \n",
    "            with respect to weights and bias (backpropagation). \n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "\n",
    "    def output_size(self):\n",
    "        \"\"\" Calculate size of this layer's output.\n",
    "        input_shape[0] is the number of samples in the input.\n",
    "        input_shape[1:] is the shape of the feature.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "\n",
    "# define a base class for loss outputs\n",
    "class Loss(object):\n",
    "\n",
    "    def loss(self, output, output_net):      # L(y,ŷ)\n",
    "        \"\"\" Calculate mean loss given real output and network output. \"\"\"\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "\n",
    "    def input_grad(self, output, output_net):   # dL(y,ŷ)/dŷ \n",
    "        \"\"\" Calculate input gradient real output and network output. \"\"\"\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "\n",
    "# define a base class for parameterized things        \n",
    "class Parameterized(object):\n",
    "    \n",
    "    def params(self):\n",
    "        \"\"\" Return parameters (by reference) \"\"\"\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "    \n",
    "    def grad_params(self):\n",
    "        \"\"\" Return accumulated gradient with respect to params. \"\"\"\n",
    "        raise NotImplementedError('This is an interface class, please use a derived instance')\n",
    "\n",
    "# define a container for providing input to the network\n",
    "# represents \"Layer\" zero, containing Data X, passing to layer 1 without doing anything\n",
    "class InputLayer(Layer):\n",
    "    \n",
    "    def __init__(self, input_shape):        \n",
    "        if not isinstance(input_shape, tuple):\n",
    "            raise ValueError(\"InputLayer requires input_shape as a tuple\")\n",
    "        self.input_shape = input_shape\n",
    "        # set_trace()\n",
    "\n",
    "    def output_size(self):\n",
    "        return self.input_shape\n",
    "    \n",
    "    def fprop(self, input):\n",
    "        return input\n",
    "    \n",
    "    def bprop(self, output_grad):\n",
    "        return output_grad\n",
    "        \n",
    "class FullyConnectedLayer(Layer, Parameterized):\n",
    "    \"\"\" A standard fully connected hidden layer, as discussed in the lecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_layer, num_units, \n",
    "                 init_stddev, activation_fun=Activation('relu')):\n",
    "        self.num_units = num_units                     #4\n",
    "        self.activation_fun = activation_fun\n",
    "        self.input_shape = input_layer.output_size()   # tuple with shape of previous layer: z0 = (n,3)\n",
    "        \n",
    "        self.W = np.random.standard_normal((self.input_shape[1],self.num_units)) * 0.1    # shape = (3,4)  (num_units_prev, num_units)\n",
    "        self.b = np.zeros((1,self.num_units))\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "\n",
    "    def output_size(self):     \n",
    "        return (self.input_shape[0], self.num_units)     # (n,4)\n",
    "    \n",
    "    def fprop(self, input):                  # z1 = h1(z0*w1 + b1)   input is z0 (n,D0), output (n,D1)\n",
    "        self.last_input = input              # store z0 (n,3) for bprop (a1 is stored in activation_fun.last_input)\n",
    "        # set_trace()\n",
    "        act = np.dot(input,self.W) + self.b    # a1 = z0*w1 + b1   (n,D1)\n",
    "        # set_trace()\n",
    "        assert(act.shape[1] == self.output_size()[1])   # (n,D1) - but n can differ with mini-batches\n",
    "        if self.activation_fun == None:\n",
    "            return act             # z1 = a1 = z0*w1 + b1 (n,D1)\n",
    "        else:\n",
    "            return self.activation_fun.fprop(act)     # z1 = h1(a1)  (n,D1) \n",
    "\n",
    "    # layer.bprop takes output_grad dL/dz1 as argument and returns dL/dz0 as result for passing to next layer \n",
    "    # going forward calculates and stores gradients of layer-weights dW1 and db1 based on actual input data\n",
    "    def bprop(self, output_grad):     # \"input gradient\" is output_grad: dL/dz1    (n,D1)\n",
    "\n",
    "        # bprop(dL/dz1)              (n,D1)\n",
    "        # dL/da1 = dL/dz1 x h'(a1)   (n,D1) (hadamard product)\n",
    "        # dL/dw1 = z0_T*dL/da1       (D0,D1)\n",
    "        # dL/db1 = dL/da1            (1,D1)\n",
    "        # dL/dz0 = dL/da1 * w1_T     (n,D0)        \n",
    "        # return dL/dz0              (n,D0)\n",
    "\n",
    "        # ??? HINT: you may have to divide the weights by n\n",
    "        #       to make gradient checking work \n",
    "        #       (since you want to divide the loss by number of inputs)        \n",
    "        \n",
    "        \n",
    "        n = output_grad.shape[0]        \n",
    "                \n",
    "        if self.activation_fun == None:\n",
    "            act_grad = output_grad   # (n,D1) dL/da1 = dL/dz0 - directly passed, assuming h'(a1) = unity matrix\n",
    "        else:\n",
    "            # dL/da1 = dL/dz1 x h'(a1)   (n,D1) = (n,D1) x (n,D1) (hadamard product)\n",
    "            act_grad = self.activation_fun.bprop(output_grad)   # performs hadamard product with a1 which was stored in actvation_fun during fprop()\n",
    "        \n",
    "        # dL/dw1 = z0_T*dL/da1       (D0,D1) = (D0,n)*(n,D1)\n",
    "        self.dW = np.dot(self.last_input.T,act_grad) # dL/dw0 = XT * dL/da0        \n",
    "        \n",
    "        # dL/db1 = dL/da0            (1,D1)  collapse from (n,D1) to (1,D1) (to be summed up, after being broadcasted)\n",
    "        self.db = act_grad.sum(axis=0, keepdims=True)\n",
    "                         \n",
    "        # dL/dz0 = dL/da1 * w1_T     (n,D0) = (n,D1)*(D1,D0)  \n",
    "        grad_input = np.dot(act_grad,self.W.T)   # ?? n*\n",
    "\n",
    "        return grad_input     # must have same shape as \"last input\" (n,D0)\n",
    "        \n",
    "    def params(self):      \n",
    "        return self.W, self.b    # returns handles / \"pointers\"\n",
    "\n",
    "    def grad_params(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "# finally we specify the interface for output layers \n",
    "# which are layers that also have a loss function\n",
    "# we will implement two output layers:\n",
    "# a Linear, and Softmax (Logistic Regression) layer\n",
    "# The difference between output layers and and normal \n",
    "# layers is that they will be called to compute the gradient\n",
    "# of the loss through input_grad(). bprop will never \n",
    "# be called on them!\n",
    "class LinearOutput(Layer, Loss):   # inherits from classes Layer and Loss\n",
    "    \"\"\" A simple linear output layer that  \n",
    "        uses a squared loss (e.g. should be used for regression)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_layer):        # is called with \"layer[-1]\", passing last preceding layer as \"input_layer\"\n",
    "        self.input_size = input_layer.output_size()\n",
    "        \n",
    "    def output_size(self):    # return scalar (loss/error)\n",
    "        return (1,)\n",
    "        \n",
    "    def fprop(self, input):   # get ŷ from upper layer, pass to loss function\n",
    "        return input\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        raise NotImplementedError(\n",
    "            'LinearOutput should only be used as the last layer of a Network'\n",
    "            + ' bprop() should thus never be called on it!'\n",
    "        )\n",
    "    \n",
    "    def input_grad(self, Y, Y_pred):            # returns dL/da3 - same dim as Y, Y_pred (n,Dlast)\n",
    "        # set_trace()\n",
    "        # n_samples = float(Y.shape[0])\n",
    "        n_samples = Y.shape[0]\n",
    "\n",
    "        loss_d = (Y_pred - Y) / n_samples        # gradient of squared loss, see summer course squared_error_grad, /n_samples because each sample contributes only marginal to total loss ?? \n",
    "        # loss_d = (Y - Y_pred)        # gradient of squared loss, see summer course squared_error_grad, /n_samples because each sample contributes only marginal to total loss ?? \n",
    "\n",
    "        return loss_d\n",
    "\n",
    "        # return np.sqrt(np.sum(0.5 * loss_d**2, axis=1))\n",
    "        # return np.mean(np.sum(loss_d, axis=1))  # sum over features / mean over samples\n",
    "\n",
    "    # given in code stub\n",
    "    def loss(self, Y, Y_pred):                   # matrix (n,d_out) \n",
    "        loss = 0.5 * np.square(Y - Y_pred)       # element-wise 0.5\n",
    "        return np.mean(np.sum(loss, axis=1))     # sum of squares over out_put nodes / features, then mean over n samples\n",
    "\n",
    "        # return np.mean(0.5 * (y_predicted - y)**2) summer course, here y only one dim   \n",
    "    \n",
    "    \n",
    "class SoftmaxOutput(Layer, Loss):\n",
    "    \"\"\" A softmax output layer that calculates \n",
    "        the negative log likelihood as loss\n",
    "        and should be used for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_layer):\n",
    "        self.input_size = input_layer.output_size()\n",
    "        \n",
    "    def output_size(self):\n",
    "        return (1,)\n",
    "    \n",
    "    def fprop(self, input):\n",
    "        return softmax(input)\n",
    "    \n",
    "    def bprop(self, output_grad):\n",
    "        raise NotImplementedError(\n",
    "            'SoftmaxOutput should only be used as the last layer of a Network'\n",
    "            + ' bprop() should thus never be called on it!'\n",
    "        )\n",
    "    \n",
    "    def input_grad(self, Y, Y_pred):    # y_pred is Ŷ / f(x) is output of last layer = a3 = z3 (no nonlinearity in last layer)\n",
    "        n_samples = Y.shape[0]\n",
    "\n",
    "        loss_d = (Y_pred - Y) / n_samples        # gradient of squared loss, see summer course squared_error_grad, \n",
    "        \n",
    "        # compute dL/da3 = dL/dŶ * dŶ/da3 = softmax(a3) - y via mathematical shortcut\n",
    "        # same calculation as for sigmoid combined with cross entropy (see paper back-prop cross entropy.pdf)\n",
    "        # n_samples = Y.shape[0]\n",
    "        # result = (Y_pred - Y) / n_samples    # y is one hot encoded\n",
    "        # result = softmax(Y_pred) - Y       # y is one hot encoded\n",
    "        return loss_d                 # shape (n,Dlast) --> passed to last layer, for further backprop\n",
    "        # \n",
    "        # TODO #######################################################\n",
    "        # TODO: implement gradient of the negative log likelihood loss\n",
    "        # TODO #######################################################\n",
    "        # HINT: since this would involve taking the log \n",
    "        #       of the softmax (which is np.exp(x)/np.sum(x, axis=1))   --> ?? rather np.exp(x)/np.sum(np.exp(x), axis=1))\n",
    "        #       this gradient computation can be simplified a lot! \n",
    "        # return np.zeros_like(Y_pred)\n",
    "        # further test modification\n",
    "\n",
    "    def loss(self, Y, Y_pred):               # Y is one hot encoded, Y_pred is output of softmax, called by fprop, called by predict before calling _loss()\n",
    "        # Y (n,10) for each row y is e.g. [0,0,0,1,0,0,0,0,0,0], \n",
    "        # Y_pred (n,10) is output of last layer, e.g. [2, 37, 3, 12, 3, 1, 2, 3, ...] \n",
    "        # out = softmax(Y_pred)   !! bug             # vector (n_out)\n",
    "        # out = Y_pred\n",
    "        # out (n,10) is e.g. [0.01, 0.01, 0.01, 0.7, 0.1,....], representing probability distribution over 10 classes\n",
    "        \n",
    "        product = Y * Y_pred           # elementwise prod --> zero for all columns, exept for one, where Y=1\n",
    "\n",
    "        loss = np.sum(product,axis=1)  # (n,) collapsing colums into one column --> one loss per row\n",
    "        \n",
    "        # to make the loss numerically stable \n",
    "        # you may want to add an epsilon in the log ;) loss could be zero, --> log explodes\n",
    "        eps = 1e-10\n",
    "\n",
    "        cross_entropy = - np.log(loss + eps)   # (n,)\n",
    "\n",
    "        return np.mean(cross_entropy)          # (1,), scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network class\n",
    "With all layers in place (and properly implemented by you) we can finally define a neural network.\n",
    "For our purposes a neural network is simply a collection of layers which we will cycle through and on which we will call fprop and bprop to compute partial derivatives with respect to the input and the parameters.\n",
    "\n",
    "Pay special attention to the *check_gradients()* function in which you should implement automatic differentiation. This function will become your best friend when checking the correctness of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\" Our Neural Network container class. # \n",
    "    \"\"\"\n",
    "    def __init__(self, layers):    # https://www.python-course.eu/python3_magic_methods.php __init is called automatically, here the method is overloaded\n",
    "        self.layers = layers\n",
    "        \n",
    "    def _loss(self, X, Y):    # single underscore used by convention to avoid conflicts with python keywords\n",
    "        Y_pred = self.predict(X)    # yields output of softmax\n",
    "        return self.layers[-1].loss(Y, Y_pred)    # loss function of last layer\n",
    "\n",
    "    def predict(self, X):    # forward pass through all layers, return Y_pred (Ŷ)   \n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        # print(\"predict\\n\")\n",
    "        next_output = X\n",
    "        # set_trace()\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            next_output = layer.fprop(next_output)    # z1 = layer.fprop(z0)\n",
    "            # print(\"predict layer {} next_output \\n{} \\n\".format(l,next_output))\n",
    "        Y_pred = next_output\n",
    "        return Y_pred         # (n,10), as returned by softmax --> to be interpreted as one hot encoded\n",
    "    # ???? next_grad OK ?? überlebt for schleife ? \n",
    "    def backpropagate(self, Y, Y_pred, upto=0):    # updating all W,b params \n",
    "        \"\"\" Backpropagation of partial derivatives through \n",
    "            the complete network up to layer 'upto'\n",
    "        \"\"\"\n",
    "        # backpropagate through all layers, start with last layer, there call input_grad() instead of bprop\n",
    "        next_grad = self.layers[-1].input_grad(Y, Y_pred)   # returns \"input_grad\" dL/da3 of upper fully connected layer\n",
    "        # set_trace()\n",
    "        # print(\"backprop input_output \\n{} \\n\".format(next_grad))\n",
    "        for layer in reversed(self.layers[upto:-1]):    # run backwards, from the second last down to \"upto\"\n",
    "            next_grad = layer.bprop(next_grad) \n",
    "            # print(\"backprop next_output \\n{} \\n\".format(next_grad))\n",
    "\n",
    "        # print(\"backprop final output \\n{} \\n\".format(next_grad))\n",
    "            \n",
    "        return next_grad       # return gradient of last layer, dL/dX (n,D)\n",
    "    \n",
    "    def classification_error(self, X, Y):\n",
    "        \"\"\" Calculate error on the given data \n",
    "            assuming they are classes that should be predicted. \n",
    "        \"\"\"\n",
    "        Y_pred = unhot(self.predict(X))     # convert softmax output vector --> digit \n",
    "        error = Y_pred != Y\n",
    "        return np.mean(error)   # error rate\n",
    "\n",
    "     \n",
    "    def update_network(self, learning_rate):        # update all parameters given current gradients\n",
    "        for lc, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Parameterized):     # only parameterized layers, not input and output_layer\n",
    "                # set_trace()\n",
    "                layer.W = layer.W - layer.dW * learning_rate\n",
    "                layer.b = layer.b - layer.db * learning_rate\n",
    "\n",
    "                # W,b   = layer.params()\n",
    "                # dW,db = layer.grad_params()\n",
    "                # print(\"layer_before {} W \\n {}\".format(lc,W))\n",
    "                # W = W - dW * learning_rate           # W,b are pointers, assignment \"W =\" acts NOT on object variable (tested)\n",
    "                # b = b - db * learning_rate\n",
    "                # set_trace()\n",
    "                # print(\"layer_after {} W \\n {}\".format(lc,W))\n",
    "                # layer.W = W\n",
    "                # layer.b = b\n",
    "                # print(\"layer_changed {} W \\n {}\".format(lc,W))\n",
    "                \n",
    "\n",
    "                \n",
    "    def sgd_epoch(self, X, Y, learning_rate, batch_size):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = n_samples // batch_size      # define the number of mini batches\n",
    "        for b in range(n_batches):\n",
    "            batch_start = b * batch_size\n",
    "            batch_end = (b + 1) * batch_size\n",
    "            \n",
    "            X_batch = X[batch_start:batch_end]\n",
    "            Y_batch = Y[batch_start:batch_end]   # if batch_end too large --> gets ignored by numpy  \n",
    "            \n",
    "            Y_batch_pred = self.predict(X_batch)\n",
    "            \n",
    "            self.backpropagate(Y_batch, Y_batch_pred)\n",
    "            \n",
    "            self.update_network(learning_rate)\n",
    "            \n",
    "            # TODO #####################################\n",
    "            # Implement stochastic gradient descent here\n",
    "            # TODO #####################################\n",
    "            # start by extracting a batch from X and Y\n",
    "            # (you can assume the inputs are already shuffled)\n",
    "\n",
    "            # TODO: then forward and backward propagation + updates\n",
    "            # HINT: layer.params() returns parameters *by reference*\n",
    "            #       so you can easily update in-place\n",
    "            # pass   \n",
    "    \n",
    "    def gd_epoch(self, X, Y, learning_rate):       # ? why no learning rate here ?\n",
    "        # first attempt: whole dataset\n",
    "        Y_pred = self.predict(X)         # predict Y based on given data and presend parameter settings\n",
    "        # print(\"predict {}\\n\".format(Y_pred))\n",
    "        self.backpropagate(Y, Y_pred)    # comparing Y and Y_pred calculate gradients of all parameters (stored in layers)\n",
    "\n",
    "        # update all parameters of all layers\n",
    "        self.update_network(learning_rate)\n",
    "        \n",
    "        # TODO ##################################################\n",
    "        # Implement batch gradient descent here\n",
    "        # A few hints:\n",
    "        #   There are two strategies you can follow:\n",
    "        #   Either shove the whole dataset throught the network\n",
    "        #   at once (which can be problematic for large datasets)\n",
    "        #   or run through it batch wise as in the sgd approach\n",
    "        #   and accumulate the gradients for all parameters as\n",
    "        #   you go through the data. Either way you should then\n",
    "        #   do one gradient step after you went through the\n",
    "        #   complete dataset!\n",
    "        # TODO ##################################################\n",
    "        # pass\n",
    "\n",
    "#   def validation_error(self, X_valid, _valid):\n",
    "#        \"\"\" Train network on the given data. \"\"\"\n",
    "#        global y_valid, X_valid\n",
    "#        n_samples = X.shape[0]                      \n",
    "        \n",
    "        \n",
    "    def train(self, X, Y, learning_rate=0.1, max_epochs=100, \n",
    "              batch_size=64, descent_type=\"sgd\", y_one_hot=True):\n",
    "        \"\"\" Train network on the given data. \"\"\"\n",
    "        n_samples = X.shape[0]              \n",
    "        n_batches = n_samples // batch_size\n",
    "        if y_one_hot:           # !! y_one_hot = False yields error, because self.predict always un_hots \n",
    "            Y_train = one_hot(Y)\n",
    "        else:\n",
    "            Y_train = Y\n",
    "        print(\"... starting training\")\n",
    "        for e in range(max_epochs+1):\n",
    "            if descent_type == \"sgd\":\n",
    "                self.sgd_epoch(X, Y_train, learning_rate, batch_size)\n",
    "            elif descent_type == \"gd\":\n",
    "                self.gd_epoch(X, Y_train, learning_rate)\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown gradient descent type {}\".format(descent_type))\n",
    "\n",
    "            # Output error on the training data\n",
    "            train_loss = self._loss(X, Y_train)             # calls one forward pass given X and computes loss on result\n",
    "            train_error = self.classification_error(X, Y)   # calls one forward pass given X and counts error-rate (in % of all data)\n",
    "\n",
    "\n",
    "            global X_valid, y_valid\n",
    "            # valid_loss = self._loss(X_valid, Y_train)     #\n",
    "            valid_error = self.classification_error(X_valid, y_valid)   # \n",
    "            # print('epoch {:.4f}, validation error {:.4f}'.format(e, valid_error))\n",
    "\n",
    "            print('epoch {:.4f}, loss {:.4f}, train error {:.4f}, valid error {:.4f}'.\\\n",
    "                  format(e, train_loss, train_error, valid_error))            \n",
    "            \n",
    "    \n",
    "    def check_gradients(self, X, Y):\n",
    "        \"\"\" Helper function to test the parameter gradients for\n",
    "        correctness. \"\"\"\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Parameterized):      # not for layers without parameters (input / output)\n",
    "                print('checking gradient for layer {}'.format(l))\n",
    "                for p, param in enumerate(layer.params()):    # layer.params() returns layer.W and layer.b (p=tuple(2,))\n",
    "                    # we iterate through all parameters, first W then b\n",
    "                    param_shape = param.shape\n",
    "\n",
    "                    def output_given_params(param_new):  # return output / loss of total network given new params\n",
    "                        \"\"\" A function that will compute the output \n",
    "                            of the network given a set of parameters\n",
    "                        \"\"\"\n",
    "                        # assign new params to param of this layer, referenced by param[:]\n",
    "                        param[:] = np.reshape(param_new, param_shape)    # \n",
    "                        # return loss, computed with these new params\n",
    "                        return self._loss(X, Y)\n",
    "\n",
    "                    def grad_given_params(param_new):     # return grad of this param (W or b) given the param\n",
    "                        \"\"\"A function that will compute the gradient \n",
    "                           of the network given a set of parameters\n",
    "                        \"\"\"\n",
    "                        # assign new param W or b to this layer, referenced by param[:]\n",
    "                        param[:] = np.reshape(param_new, param_shape)\n",
    "                        \n",
    "                        Y_pred = self.predict(X)               # prediction based on new params, softmax output vector (n,10)\n",
    "                        # backpropagate given new Y_pred, update all gradients dW, db up to this layer \n",
    "                        # set_trace()\n",
    "                        self.backpropagate(Y, Y_pred, upto=l)  # Backpropagation of partial derivatives \n",
    "                        # return the computed gradient W or b of this layer\n",
    "                        return np.ravel(self.layers[l].grad_params()[p])\n",
    "\n",
    "                    # let the initial parameters be the ones that\n",
    "                    # are currently placed in the network and flatten them\n",
    "                    # to a vector for convenient comparisons, printing etc.\n",
    "                    param_init = np.ravel(np.copy(param))\n",
    "                    # set_trace()\n",
    "\n",
    "                    epsilon = 1e-4\n",
    "                    # sci_err = scipy.optimize.check_grad(output_given_params, grad_given_params, param_init, epsilon=epsilon)\n",
    "                    # print('scipy_grad_error {:.2e} \\n'.format(sci_err))                    \n",
    "                    \n",
    "                    loss_base = output_given_params(param_init)    # scalar loss at current params\n",
    "                    # set_trace()\n",
    "\n",
    "                    # for each single element of W,b make eps step and calculate finite difference\n",
    "                    gparam_fd = np.zeros_like(param_init)    # gradient calculated through finited differences\n",
    "                    new_param = np.zeros_like(param_init)    # new, eps-deviated param for testing\n",
    "                    for i in range(param_init.shape[0]):     # go through all rows and colums of this W or b \n",
    "                        # for j in range(param.shape[1]):\n",
    "                        new_param[:] = param_init            # set back to orignial value\n",
    "\n",
    "                        new_param[i] = param_init[i] + epsilon    # vary only for this element\n",
    "                        loss_plus = output_given_params(new_param)           \n",
    "\n",
    "#                        new_param[i] = param_init[i] - epsilon\n",
    "#                        loss_minus = output_given_params(new_param)\n",
    "                            \n",
    "                        gparam_fd[i] = (loss_plus - loss_base) / (epsilon)   # dfferential --> approximated gradient                            \n",
    "                    \n",
    "                    gparam_bprop = grad_given_params(param_init)  # gradient als calculated by bprop\n",
    "                    # print(\"gparam_bprop {}\\n\".format(gparam_bprop))\n",
    "                    # print(\"gparam_fd {}\\n\".format(gparam_fd))\n",
    "                    # print(\"diff {}\\n\".format(gparam_bprop/gparam_fd))\n",
    "\n",
    "\n",
    "                    # err = np.mean(np.abs(gparam_bprop - gparam_fd))   Scipi grad check takes L2 error\n",
    "                    # set_trace()\n",
    "                    err = np.sqrt(np.sum((gparam_bprop - gparam_fd)**2))  # as SciPi \n",
    "                    print('grad_error {:.2e}'.format(err))\n",
    "                    # if err > epsilon:\n",
    "                        # print(\"error too large\")\n",
    "                    # assert(err < epsilon)\n",
    "                    \n",
    "                    # reset the parameters to their initial values\n",
    "                    param[:] = np.reshape(param_init, param_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n",
      "... starting training\n",
      "epoch 0.0000, loss 1.0842, train error 0.5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10000,784) and (3,5) not aligned: 784 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-5ea33cbd178a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#nn_mini.check_gradients(X, y_one_hot)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m nn_mini.train(X, y, learning_rate=0.1, \n\u001b[0;32m---> 35\u001b[0;31m               max_epochs=30, batch_size=2, descent_type=\"sgd\", y_one_hot=True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-188-6c3927079dff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, max_epochs, batch_size, descent_type, y_one_hot)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mglobal\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;31m# valid_loss = self._loss(X_valid, Y_train)     #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mvalid_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {:.4f}, validation error {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-6c3927079dff>\u001b[0m in \u001b[0;36mclassification_error\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0massuming\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munhot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# convert softmax output vector --> digit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# error rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-188-6c3927079dff>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mnext_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_output\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# z1 = layer.fprop(z0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m# print(\"predict layer {} next_output \\n{} \\n\".format(l,next_output))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-187-f70c58518f7c>\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m              \u001b[0;31m# store z0 (n,3) for bprop (a1 is stored in activation_fun.last_input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m    \u001b[0;31m# a1 = z0*w1 + b1   (n,D1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (n,D1) - but n can differ with mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10000,784) and (3,5) not aligned: 784 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "# first super-mini network for preliminary tests\n",
    "X = np.array([[1.,2.,3.],[2.,5.,3.],[2.,4.,6.],[1.,2.,3.]])   \n",
    "y = np.array([1,2,0,1])\n",
    "\n",
    "mini_layers = [InputLayer(X.shape)]\n",
    "mini_layers.append(FullyConnectedLayer(\n",
    "                mini_layers[-1],      \n",
    "                num_units=5,    \n",
    "                init_stddev=0.1,\n",
    "                activation_fun=Activation('relu')\n",
    "                # activation_fun = None    \n",
    "))\n",
    "mini_layers.append(FullyConnectedLayer(\n",
    "                mini_layers[-1],      \n",
    "                num_units=2,    \n",
    "                init_stddev=0.1,\n",
    "                activation_fun=Activation('sigmoid')\n",
    "                # activation_fun = None    \n",
    "))\n",
    "mini_layers.append(FullyConnectedLayer(\n",
    "                mini_layers[-1],      \n",
    "                num_units=3,    # number of classes = number of different digits in y\n",
    "                init_stddev=0.1,\n",
    "                # activation_fun=Activation('relu')\n",
    "                activation_fun = None    \n",
    "))\n",
    "mini_layers.append(SoftmaxOutput(mini_layers[-1]))\n",
    "# mini_layers.append(LinearOutput(mini_layers[-1]))\n",
    "\n",
    "nn_mini = NeuralNetwork(mini_layers)\n",
    "y_one_hot = one_hot(y)\n",
    "print(y_one_hot)\n",
    "#nn_mini.check_gradients(X, y_one_hot)\n",
    "nn_mini.train(X, y, learning_rate=0.1, \n",
    "              max_epochs=30, batch_size=2, descent_type=\"sgd\", y_one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "After implementing everything it is always a good idea to setup some layers and perform gradient\n",
    "checking on random data. **Note** that this is only an example! It is not a useful network architecture ;). We also expect you to play around with this to test all your implemented components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (5, 10)\n",
    "n_labels = 6\n",
    "layers = [InputLayer(input_shape)]\n",
    "\n",
    "layers.append(FullyConnectedLayer(\n",
    "                layers[-1],      \n",
    "                num_units=15,\n",
    "                init_stddev=0.1,\n",
    "                activation_fun=Activation('relu')\n",
    "))\n",
    "layers.append(FullyConnectedLayer(\n",
    "                layers[-1],\n",
    "                num_units=6,\n",
    "                init_stddev=0.1,\n",
    "                activation_fun=Activation('tanh')\n",
    "))\n",
    "layers.append(FullyConnectedLayer(\n",
    "                layers[-1],\n",
    "                num_units=n_labels,\n",
    "                init_stddev=0.1,\n",
    "                activation_fun=Activation('relu')\n",
    "))\n",
    "layers.append(SoftmaxOutput(layers[-1]))\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random data\n",
    "X = np.random.normal(size=input_shape)\n",
    "# and random labels\n",
    "Y = np.zeros((input_shape[0], n_labels))\n",
    "for i in range(Y.shape[0]):\n",
    "    idx = np.random.randint(n_labels)\n",
    "    Y[i, idx] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking gradient for layer 1\n",
      "grad_error 6.08e-04\n",
      "grad_error 2.65e-08\n",
      "checking gradient for layer 2\n",
      "grad_error 6.32e-08\n",
      "grad_error 3.22e-07\n",
      "checking gradient for layer 3\n",
      "grad_error 1.05e-07\n",
      "grad_error 9.54e-06\n"
     ]
    }
   ],
   "source": [
    "nn.check_gradients(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST\n",
    "Finally we can let our network run on the MNIST dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load the data and reshape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... done loading data\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "Dtrain, Dval, Dtest = mnist()\n",
    "X_train, y_train = Dtrain\n",
    "X_valid, y_valid = Dval\n",
    "X_test,  y_test  = Dtest\n",
    "# Downsample training data to make it a bit faster for testing this code\n",
    "n_train_samples = 100000\n",
    "n_valid_samples = 100000\n",
    "n_test_samples = 100000\n",
    "\n",
    "train_idxs = np.random.permutation(X_train.shape[0])[:n_train_samples]\n",
    "X_train = X_train[train_idxs]\n",
    "y_train = y_train[train_idxs]\n",
    "\n",
    "valid_idxs = np.random.permutation(X_valid.shape[0])[:n_valid_samples]\n",
    "X_valid = X_valid[valid_idxs]\n",
    "y_valid = y_valid[valid_idxs]\n",
    "\n",
    "test_idxs = np.random.permutation(X_test.shape[0])[:n_test_samples]\n",
    "X_test = X_test[test_idxs]\n",
    "y_test = y_test[test_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dtrain* contains 50k images which are of size 28 x 28 pixels. Hence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 784)\n",
      "y_train shape: (50000,)\n",
      "y_valid shape: (10000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: {}\".format(np.shape(X_train)))\n",
    "print(\"y_train shape: {}\".format(np.shape(y_train)))\n",
    "\n",
    "print(\"y_valid shape: {}\".format(np.shape(y_valid)))\n",
    "print(\"y_test shape: {}\".format(np.shape(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train will automatically be converted in the *train()* function to one_hot encoding.\n",
    "\n",
    "\n",
    "But we need to reshape X_train, as our Network expects flat vectors of size 28*28 as input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X_train size: (50000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "print(\"Reshaped X_train size: {}\".format(X_train.shape))\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, much better ;-)! \n",
    "\n",
    "Now we can finally really start training a Network!\n",
    "\n",
    "\n",
    "I pre-defined a small Network for you below. Again This is not really a good default and will not produce state of the art results. Please play around with this a bit. See how different activation functions and training procedures (gd / sgd) affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... starting training\n",
      "epoch 0.0000, loss 0.1316, train error 0.0394, valid error 0.0408\n",
      "epoch 1.0000, loss 0.0873, train error 0.0275, valid error 0.0335\n",
      "epoch 2.0000, loss 0.0657, train error 0.0212, valid error 0.0317\n",
      "epoch 3.0000, loss 0.0555, train error 0.0183, valid error 0.0302\n",
      "epoch 4.0000, loss 0.0425, train error 0.0138, valid error 0.0287\n",
      "epoch 5.0000, loss 0.0369, train error 0.0128, valid error 0.0299\n",
      "epoch 6.0000, loss 0.0315, train error 0.0107, valid error 0.0290\n",
      "epoch 7.0000, loss 0.0238, train error 0.0081, valid error 0.0265\n",
      "epoch 8.0000, loss 0.0160, train error 0.0054, valid error 0.0269\n",
      "epoch 9.0000, loss 0.0136, train error 0.0045, valid error 0.0271\n",
      "epoch 10.0000, loss 0.0134, train error 0.0042, valid error 0.0268\n",
      "Duration: 92.2s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Setup a small MLP / Neural Network\n",
    "# we can set the first shape to None here to indicate that\n",
    "# we will input a variable number inputs to the network\n",
    "input_shape = (None, 28*28)\n",
    "layers = [InputLayer(input_shape)]\n",
    "layers.append(FullyConnectedLayer(\n",
    "                layers[-1],\n",
    "                num_units=100,\n",
    "                init_stddev=0.01,\n",
    "                activation_fun=Activation('tanh')\n",
    "))\n",
    "layers.append(FullyConnectedLayer(\n",
    "                layers[-1],\n",
    "                num_units=100,\n",
    "                init_stddev=0.01,\n",
    "                activation_fun=Activation('tanh')\n",
    "))\n",
    "layers.append(FullyConnectedLayer(\n",
    "                layers[-1],\n",
    "                num_units=10,        # last layer has 10 units, one for each figure --> one hot encoding\n",
    "                init_stddev=0.01,\n",
    "                # last layer has no nonlinearity \n",
    "                # (softmax will be applied in the output layer)\n",
    "                activation_fun=None \n",
    "))\n",
    "layers.append(SoftmaxOutput(layers[-1]))\n",
    "\n",
    "nn = NeuralNetwork(layers)\n",
    "# Train neural network\n",
    "t0 = time.time()\n",
    "nn.train(X_train, y_train, learning_rate=0.1, \n",
    "         max_epochs=10, batch_size=10, descent_type=\"sgd\", y_one_hot=True)\n",
    "t1 = time.time()\n",
    "print('Duration: {:.1f}s'.format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Figure out a reasonable Network that achieves good performance\n",
    "As the last part of this task, setup a network that works well and gets reasonable accuracy, say ~ 1-3 percent error on the **validation set**. \n",
    "Train this network on the complete data and compute the **test error**. \n",
    "\n",
    "Once you have done this, visualize a few digits from the the test set that the network gets right as well as a few that the network gets wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
