{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.misc import imread\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras import backend as K\n",
    "\n",
    "from utilities import *\n",
    "from Keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n"
     ]
    }
   ],
   "source": [
    "prepare_environment(num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "whale_class_map:\n",
      "{'w_7554f44': 3, 'w_fd1cb9d': 6, 'w_98baff9': 4, 'w_ab4cae2': 5, 'w_693c9ee': 2, 'w_1eafe46': 1, 'w_1287fbc': 0}\n",
      "class_whale_map:\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "print_data_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 135s - loss: 12.7834 - acc: 0.1081 - val_loss: 13.6384 - val_acc: 0.1538\n",
      "unfrozen 2 top CNN layers\n",
      "Epoch 1/1\n",
      " - 157s - loss: 13.4887 - val_loss: 13.6384\n"
     ]
    }
   ],
   "source": [
    "histories = train_and_save(model, epochs=1, cnn_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layers = True\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# fully-connected layer on top\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "if two_layers:\n",
    "    # new added as https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "# logistic layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# metrics='accuracy' causes the model to store and report accuracy (train and validate)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_4\n",
      "1 conv2d_283\n",
      "2 batch_normalization_283\n",
      "3 activation_283\n",
      "4 conv2d_284\n",
      "5 batch_normalization_284\n",
      "6 activation_284\n",
      "7 conv2d_285\n",
      "8 batch_normalization_285\n",
      "9 activation_285\n",
      "10 max_pooling2d_13\n",
      "11 conv2d_286\n",
      "12 batch_normalization_286\n",
      "13 activation_286\n",
      "14 conv2d_287\n",
      "15 batch_normalization_287\n",
      "16 activation_287\n",
      "17 max_pooling2d_14\n",
      "18 conv2d_291\n",
      "19 batch_normalization_291\n",
      "20 activation_291\n",
      "21 conv2d_289\n",
      "22 conv2d_292\n",
      "23 batch_normalization_289\n",
      "24 batch_normalization_292\n",
      "25 activation_289\n",
      "26 activation_292\n",
      "27 average_pooling2d_28\n",
      "28 conv2d_288\n",
      "29 conv2d_290\n",
      "30 conv2d_293\n",
      "31 conv2d_294\n",
      "32 batch_normalization_288\n",
      "33 batch_normalization_290\n",
      "34 batch_normalization_293\n",
      "35 batch_normalization_294\n",
      "36 activation_288\n",
      "37 activation_290\n",
      "38 activation_293\n",
      "39 activation_294\n",
      "40 mixed0\n",
      "41 conv2d_298\n",
      "42 batch_normalization_298\n",
      "43 activation_298\n",
      "44 conv2d_296\n",
      "45 conv2d_299\n",
      "46 batch_normalization_296\n",
      "47 batch_normalization_299\n",
      "48 activation_296\n",
      "49 activation_299\n",
      "50 average_pooling2d_29\n",
      "51 conv2d_295\n",
      "52 conv2d_297\n",
      "53 conv2d_300\n",
      "54 conv2d_301\n",
      "55 batch_normalization_295\n",
      "56 batch_normalization_297\n",
      "57 batch_normalization_300\n",
      "58 batch_normalization_301\n",
      "59 activation_295\n",
      "60 activation_297\n",
      "61 activation_300\n",
      "62 activation_301\n",
      "63 mixed1\n",
      "64 conv2d_305\n",
      "65 batch_normalization_305\n",
      "66 activation_305\n",
      "67 conv2d_303\n",
      "68 conv2d_306\n",
      "69 batch_normalization_303\n",
      "70 batch_normalization_306\n",
      "71 activation_303\n",
      "72 activation_306\n",
      "73 average_pooling2d_30\n",
      "74 conv2d_302\n",
      "75 conv2d_304\n",
      "76 conv2d_307\n",
      "77 conv2d_308\n",
      "78 batch_normalization_302\n",
      "79 batch_normalization_304\n",
      "80 batch_normalization_307\n",
      "81 batch_normalization_308\n",
      "82 activation_302\n",
      "83 activation_304\n",
      "84 activation_307\n",
      "85 activation_308\n",
      "86 mixed2\n",
      "87 conv2d_310\n",
      "88 batch_normalization_310\n",
      "89 activation_310\n",
      "90 conv2d_311\n",
      "91 batch_normalization_311\n",
      "92 activation_311\n",
      "93 conv2d_309\n",
      "94 conv2d_312\n",
      "95 batch_normalization_309\n",
      "96 batch_normalization_312\n",
      "97 activation_309\n",
      "98 activation_312\n",
      "99 max_pooling2d_15\n",
      "100 mixed3\n",
      "101 conv2d_317\n",
      "102 batch_normalization_317\n",
      "103 activation_317\n",
      "104 conv2d_318\n",
      "105 batch_normalization_318\n",
      "106 activation_318\n",
      "107 conv2d_314\n",
      "108 conv2d_319\n",
      "109 batch_normalization_314\n",
      "110 batch_normalization_319\n",
      "111 activation_314\n",
      "112 activation_319\n",
      "113 conv2d_315\n",
      "114 conv2d_320\n",
      "115 batch_normalization_315\n",
      "116 batch_normalization_320\n",
      "117 activation_315\n",
      "118 activation_320\n",
      "119 average_pooling2d_31\n",
      "120 conv2d_313\n",
      "121 conv2d_316\n",
      "122 conv2d_321\n",
      "123 conv2d_322\n",
      "124 batch_normalization_313\n",
      "125 batch_normalization_316\n",
      "126 batch_normalization_321\n",
      "127 batch_normalization_322\n",
      "128 activation_313\n",
      "129 activation_316\n",
      "130 activation_321\n",
      "131 activation_322\n",
      "132 mixed4\n",
      "133 conv2d_327\n",
      "134 batch_normalization_327\n",
      "135 activation_327\n",
      "136 conv2d_328\n",
      "137 batch_normalization_328\n",
      "138 activation_328\n",
      "139 conv2d_324\n",
      "140 conv2d_329\n",
      "141 batch_normalization_324\n",
      "142 batch_normalization_329\n",
      "143 activation_324\n",
      "144 activation_329\n",
      "145 conv2d_325\n",
      "146 conv2d_330\n",
      "147 batch_normalization_325\n",
      "148 batch_normalization_330\n",
      "149 activation_325\n",
      "150 activation_330\n",
      "151 average_pooling2d_32\n",
      "152 conv2d_323\n",
      "153 conv2d_326\n",
      "154 conv2d_331\n",
      "155 conv2d_332\n",
      "156 batch_normalization_323\n",
      "157 batch_normalization_326\n",
      "158 batch_normalization_331\n",
      "159 batch_normalization_332\n",
      "160 activation_323\n",
      "161 activation_326\n",
      "162 activation_331\n",
      "163 activation_332\n",
      "164 mixed5\n",
      "165 conv2d_337\n",
      "166 batch_normalization_337\n",
      "167 activation_337\n",
      "168 conv2d_338\n",
      "169 batch_normalization_338\n",
      "170 activation_338\n",
      "171 conv2d_334\n",
      "172 conv2d_339\n",
      "173 batch_normalization_334\n",
      "174 batch_normalization_339\n",
      "175 activation_334\n",
      "176 activation_339\n",
      "177 conv2d_335\n",
      "178 conv2d_340\n",
      "179 batch_normalization_335\n",
      "180 batch_normalization_340\n",
      "181 activation_335\n",
      "182 activation_340\n",
      "183 average_pooling2d_33\n",
      "184 conv2d_333\n",
      "185 conv2d_336\n",
      "186 conv2d_341\n",
      "187 conv2d_342\n",
      "188 batch_normalization_333\n",
      "189 batch_normalization_336\n",
      "190 batch_normalization_341\n",
      "191 batch_normalization_342\n",
      "192 activation_333\n",
      "193 activation_336\n",
      "194 activation_341\n",
      "195 activation_342\n",
      "196 mixed6\n",
      "197 conv2d_347\n",
      "198 batch_normalization_347\n",
      "199 activation_347\n",
      "200 conv2d_348\n",
      "201 batch_normalization_348\n",
      "202 activation_348\n",
      "203 conv2d_344\n",
      "204 conv2d_349\n",
      "205 batch_normalization_344\n",
      "206 batch_normalization_349\n",
      "207 activation_344\n",
      "208 activation_349\n",
      "209 conv2d_345\n",
      "210 conv2d_350\n",
      "211 batch_normalization_345\n",
      "212 batch_normalization_350\n",
      "213 activation_345\n",
      "214 activation_350\n",
      "215 average_pooling2d_34\n",
      "216 conv2d_343\n",
      "217 conv2d_346\n",
      "218 conv2d_351\n",
      "219 conv2d_352\n",
      "220 batch_normalization_343\n",
      "221 batch_normalization_346\n",
      "222 batch_normalization_351\n",
      "223 batch_normalization_352\n",
      "224 activation_343\n",
      "225 activation_346\n",
      "226 activation_351\n",
      "227 activation_352\n",
      "228 mixed7\n",
      "229 conv2d_355\n",
      "230 batch_normalization_355\n",
      "231 activation_355\n",
      "232 conv2d_356\n",
      "233 batch_normalization_356\n",
      "234 activation_356\n",
      "235 conv2d_353\n",
      "236 conv2d_357\n",
      "237 batch_normalization_353\n",
      "238 batch_normalization_357\n",
      "239 activation_353\n",
      "240 activation_357\n",
      "241 conv2d_354\n",
      "242 conv2d_358\n",
      "243 batch_normalization_354\n",
      "244 batch_normalization_358\n",
      "245 activation_354\n",
      "246 activation_358\n",
      "247 max_pooling2d_16\n",
      "248 mixed8\n",
      "249 conv2d_363\n",
      "250 batch_normalization_363\n",
      "251 activation_363\n",
      "252 conv2d_360\n",
      "253 conv2d_364\n",
      "254 batch_normalization_360\n",
      "255 batch_normalization_364\n",
      "256 activation_360\n",
      "257 activation_364\n",
      "258 conv2d_361\n",
      "259 conv2d_362\n",
      "260 conv2d_365\n",
      "261 conv2d_366\n",
      "262 average_pooling2d_35\n",
      "263 conv2d_359\n",
      "264 batch_normalization_361\n",
      "265 batch_normalization_362\n",
      "266 batch_normalization_365\n",
      "267 batch_normalization_366\n",
      "268 conv2d_367\n",
      "269 batch_normalization_359\n",
      "270 activation_361\n",
      "271 activation_362\n",
      "272 activation_365\n",
      "273 activation_366\n",
      "274 batch_normalization_367\n",
      "275 activation_359\n",
      "276 mixed9_0\n",
      "277 concatenate_7\n",
      "278 activation_367\n",
      "279 mixed9\n",
      "280 conv2d_372\n",
      "281 batch_normalization_372\n",
      "282 activation_372\n",
      "283 conv2d_369\n",
      "284 conv2d_373\n",
      "285 batch_normalization_369\n",
      "286 batch_normalization_373\n",
      "287 activation_369\n",
      "288 activation_373\n",
      "289 conv2d_370\n",
      "290 conv2d_371\n",
      "291 conv2d_374\n",
      "292 conv2d_375\n",
      "293 average_pooling2d_36\n",
      "294 conv2d_368\n",
      "295 batch_normalization_370\n",
      "296 batch_normalization_371\n",
      "297 batch_normalization_374\n",
      "298 batch_normalization_375\n",
      "299 conv2d_376\n",
      "300 batch_normalization_368\n",
      "301 activation_370\n",
      "302 activation_371\n",
      "303 activation_374\n",
      "304 activation_375\n",
      "305 batch_normalization_376\n",
      "306 activation_368\n",
      "307 mixed9_1\n",
      "308 concatenate_8\n",
      "309 activation_376\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 155s - loss: 2.0515 - val_loss: 1.9929\n"
     ]
    }
   ],
   "source": [
    "histories = train_and_save(model, epochs=1, cnn_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'loss': [8.7449762486229261], 'val_loss': [11.758036809089857], 'acc': [0.17355371913141457], 'val_acc': [0.12820512935137138]}]\n"
     ]
    }
   ],
   "source": [
    "print(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"run-{}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1e6c7eb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1e5a017f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_learning_curves(histories[0], run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_dict(histories[0], keys=['loss', 'acc', 'val_loss', 'val_acc'], filename=run_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_test\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_test  validation:  None\n",
      "176  images copied as training data\n",
      "0  images copied as validation data\n",
      "write csv file with training data: data/model_test.csv\n",
      "Found 176 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# try to verify on test data --> no success so far\n",
    "\n",
    "# use all training data of the first num_classes (7) whales as test data.\n",
    "# no good practice, but all training data have been augmented, so at least some indication\n",
    "# about predictive power of model\n",
    "test_dir = \"data/model_test\"\n",
    "test_csv = \"data/model_test.csv\"\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "    sel_whales = np.arange(1,7+1),  # whales to be considered\n",
    "    all_train_dir = all_train_dir,\n",
    "    all_train_csv = all_train_csv,\n",
    "    train_dir = test_dir,\n",
    "    train_csv = test_csv,\n",
    "    valid_dir = None,     # no validation, copy all data into test_dir \"data/model_test\"\n",
    "    valid_csv = None,\n",
    "    train_valid = 1.,\n",
    "    sub_dirs = True) \n",
    "\n",
    "# for test Purposes !!!\n",
    "\n",
    "# valid_gen = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,\n",
    "    # preprocessing_function=preprocess_input,   # model specific function\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "test_flow = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    # color_mode = \"grayscale\",\n",
    "    batch_size = batch_size,     \n",
    "    target_size = (299,299),\n",
    "    class_mode = \"categorical\")    # use \"None\" ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 199s 18s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_flow, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whale_class_map:\n",
      "{'w_7554f44': 3, 'w_fd1cb9d': 6, 'w_98baff9': 4, 'w_ab4cae2': 5, 'w_693c9ee': 2, 'w_1eafe46': 1, 'w_1287fbc': 0}\n",
      "class_whale_map:\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n",
      "preds.shape:\n",
      "(176, 7)\n",
      "preds[:10]\n",
      "[[  4.15698326e-17   5.37059903e-01   1.47203204e-11   9.10298052e-12\n",
      "    4.62940127e-01   6.88846021e-14   5.20517223e-13]\n",
      " [  1.39071478e-17   5.52621305e-01   7.50012032e-12   4.14585597e-12\n",
      "    4.47378725e-01   2.65379573e-14   2.25911480e-13]\n",
      " [  7.05168042e-12   5.57020426e-01   3.60054067e-08   1.80758786e-08\n",
      "    4.42979544e-01   8.96583530e-10   3.68721698e-09]\n",
      " [  1.06108460e-12   5.58171093e-01   8.75452066e-09   4.85269291e-09\n",
      "    4.41828936e-01   2.11922341e-10   6.63735955e-10]\n",
      " [  1.00001886e-12   5.12859643e-01   9.93488491e-09   6.74927225e-09\n",
      "    4.87140417e-01   2.19560550e-10   6.82743306e-10]\n",
      " [  1.60271752e-13   6.33110404e-01   2.40786968e-09   1.12872867e-09\n",
      "    3.66889656e-01   4.61076524e-11   1.95034544e-10]\n",
      " [  1.96587744e-18   5.48468530e-01   1.84802390e-12   1.22766803e-12\n",
      "    4.51531529e-01   6.05272803e-15   4.43070568e-14]\n",
      " [  8.94034508e-18   4.89274710e-01   5.63482681e-12   3.54508068e-12\n",
      "    5.10725260e-01   2.13034190e-14   1.94235388e-13]\n",
      " [  1.17751230e-12   5.96255422e-01   9.45762668e-09   6.39914255e-09\n",
      "    4.03744519e-01   1.79183307e-10   7.76524789e-10]\n",
      " [  2.08562529e-12   5.99593461e-01   1.39642209e-08   9.61460955e-09\n",
      "    4.00406539e-01   3.46243617e-10   1.18190391e-09]]\n",
      "model predictions: \n",
      " [['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_98baff9' 'w_1eafe46' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44' 'w_fd1cb9d']]\n",
      "true labels \n",
      " ['w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc']\n",
      "24 true predictions out of 176: accurracy: 0.1364 \n",
      "MAP 0.313352272727\n",
      "Dummy MAP averaged 0.2702\n"
     ]
    }
   ],
   "source": [
    "whale_class_map = (test_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=test_dir) # get dict mapping class_no --> whalenames\n",
    "print(\"whale_class_map:\")\n",
    "print(whale_class_map)\n",
    "print(\"class_whale_map:\")\n",
    "print(class_whale_map)\n",
    "print(\"preds.shape:\")\n",
    "print(preds.shape)\n",
    "print(\"preds[:10]\")\n",
    "print(preds[:10])\n",
    "\n",
    "# get list of model predictions: one ordered list of maxpred whalenames per image\n",
    "top_k = preds.argsort()[:, -max_preds:][:, ::-1]\n",
    "model_preds = [([class_whale_map[i] for i in line]) for line in top_k]  \n",
    "\n",
    "# get list of true labels: one whalename per image\n",
    "test_list = read_csv(file_name = test_csv)    # list with (filename, whalename)\n",
    "true_labels = []\n",
    "for fn in test_flow.filenames:\n",
    "    offset, filename = fn.split('/')\n",
    "    whale = [line[1] for line in test_list if line[0]==filename][0]\n",
    "    true_labels.append(whale)\n",
    "\n",
    "print(\"model predictions: \\n\", np.array(model_preds)[0:10])\n",
    "print(\"true labels \\n\", np.array(true_labels)[0:10])\n",
    "\n",
    "# compute accuracy by hand - List of True Predictions \n",
    "TP_List = [(1 if model_preds[i][0]==true_labels[i] else 0) for i in range(len(true_labels))]\n",
    "acc = np.sum(TP_List)/len(TP_List)\n",
    "print(\"{} true predictions out of {}: accurracy: {:0.4f} \".format(np.sum(TP_List),len(TP_List),acc))\n",
    "\n",
    "MAP = mean_average_precision(model_preds, true_labels, max_preds)\n",
    "print(\"MAP\", MAP)\n",
    "\n",
    "# run Dummy MAP generator 5 times and get average for more reliable result\n",
    "avg_dummy_map = np.mean([Dummy_MAP(probs = 'weighted', distributed_as = train_csv, image_no = len(TP_List)) for i in range(5)])\n",
    "print(\"Dummy MAP averaged {:0.4f}\".format(avg_dummy_map))\n",
    "\n",
    "# MAP only slightly higher than averag dummy MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_test_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "120  images copied as training data\n",
      "56  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "all_train_dir = \"data/train\"     # directory with original kaggle training data\n",
    "all_train_csv = \"data/train.csv\" # original kaggle train.csv file\n",
    "train_dir = \"data/model_train\"\n",
    "train_csv = \"data/model_train.csv\"\n",
    "valid_dir = \"data/model_valid\"\n",
    "valid_csv = \"data/model_valid.csv\"\n",
    "\n",
    "num_classes = 7     # number of whales to be considered (in order of occuurence)\n",
    "max_preds = 5       # number of ranked predictions (default 5)\n",
    "batch_size = 16     # used for training as well as validation\n",
    "train_valid = 0.7   # ratio training / validation data\n",
    "\n",
    "# create training environment for training data\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = train_dir,\n",
    "       train_csv = train_csv,\n",
    "       valid_dir = valid_dir,\n",
    "       valid_csv = valid_csv,\n",
    "       train_valid = train_valid,\n",
    "       sub_dirs = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)                     # new added as https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "# and a logistic layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "# metrics='accuracy' causes the model to store and report accuracy (train and validate)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 120 images belonging to 7 classes.\n",
      "Found 56 images belonging to 7 classes.\n",
      "{'w_fd1cb9d': 6, 'w_ab4cae2': 5, 'w_1287fbc': 0, 'w_7554f44': 3, 'w_98baff9': 4, 'w_1eafe46': 1, 'w_693c9ee': 2}\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "# define image generator\n",
    "train_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,   # redundant with featurewise_center ? \n",
    "    # preprocessing_function=preprocess_input, not used in most examples\n",
    "    # horizontal_flip = True,    # no, as individual shapes are looked for\n",
    "    fill_mode = \"nearest\",\n",
    "    zoom_range = 0.3,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range=0.3,\n",
    "    rotation_range=30)\n",
    "\n",
    "train_flow = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    # save_to_dir = \"data/model_train/augmented\",    \n",
    "    # color_mode = \"grayscale\",\n",
    "    target_size = (299,299),\n",
    "    batch_size = batch_size, \n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "valid_gen = image.ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "valid_flow = valid_gen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size = (299,299),\n",
    "    batch_size = batch_size,        \n",
    "    class_mode = \"categorical\")\n",
    "\n",
    "whale_class_map = (train_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=train_dir) # get dict mapping class_no --> whalenames\n",
    "print(whale_class_map)\n",
    "print(class_whale_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'InceptV3_1_Epochs_7_classes_2nd.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fcd16e822f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reload model that gets not stuck on training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'InceptV3_1_Epochs_7_classes_2nd.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/Whales-4YGaJ5HX/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;31m# instantiate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Whales-4YGaJ5HX/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Whales-4YGaJ5HX/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'InceptV3_1_Epochs_7_classes_2nd.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# reload model that gets not stuck on training\n",
    "# from keras.models import load_model\n",
    "# model = load_model('InceptV3_1_Epochs_7_classes_2nd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 2s - loss: 1.4911 - acc: 0.4483 - val_loss: 1.9023 - val_acc: 0.1667\n",
      "Epoch 2/50\n",
      " - 2s - loss: 1.7744 - acc: 0.3053 - val_loss: 1.9599 - val_acc: 0.1750\n",
      "Epoch 3/50\n",
      " - 2s - loss: 1.6405 - acc: 0.3571 - val_loss: 2.1624 - val_acc: 0.2000\n",
      "Epoch 4/50\n",
      " - 2s - loss: 1.4865 - acc: 0.4032 - val_loss: 1.7803 - val_acc: 0.3750\n",
      "Epoch 5/50\n",
      " - 2s - loss: 1.5338 - acc: 0.4357 - val_loss: 1.8106 - val_acc: 0.2917\n",
      "Epoch 6/50\n",
      " - 2s - loss: 1.3621 - acc: 0.5078 - val_loss: 1.6907 - val_acc: 0.3250\n",
      "Epoch 7/50\n",
      " - 2s - loss: 1.6758 - acc: 0.4561 - val_loss: 2.0843 - val_acc: 0.1500\n",
      "Epoch 8/50\n",
      " - 1s - loss: 1.4857 - acc: 0.4651 - val_loss: 1.7196 - val_acc: 0.4250\n",
      "Epoch 9/50\n",
      " - 2s - loss: 1.5109 - acc: 0.3996 - val_loss: 1.9170 - val_acc: 0.3542\n",
      "Epoch 10/50\n",
      " - 2s - loss: 1.3692 - acc: 0.5168 - val_loss: 1.7789 - val_acc: 0.2250\n",
      "Epoch 11/50\n",
      " - 2s - loss: 1.3271 - acc: 0.5024 - val_loss: 1.9283 - val_acc: 0.3750\n",
      "Epoch 12/50\n",
      " - 2s - loss: 1.1274 - acc: 0.5697 - val_loss: 1.6438 - val_acc: 0.3000\n",
      "Epoch 13/50\n",
      " - 2s - loss: 1.1703 - acc: 0.5536 - val_loss: 1.6732 - val_acc: 0.2708\n",
      "Epoch 14/50\n",
      " - 2s - loss: 1.3279 - acc: 0.5439 - val_loss: 2.2445 - val_acc: 0.1250\n",
      "Epoch 15/50\n",
      " - 2s - loss: 1.0883 - acc: 0.6340 - val_loss: 1.6172 - val_acc: 0.4250\n",
      "Epoch 16/50\n",
      " - 1s - loss: 1.0274 - acc: 0.6611 - val_loss: 2.0734 - val_acc: 0.1500\n",
      "Epoch 17/50\n",
      " - 2s - loss: 1.1557 - acc: 0.5446 - val_loss: 1.7085 - val_acc: 0.2500\n",
      "Epoch 18/50\n",
      " - 2s - loss: 1.4609 - acc: 0.5052 - val_loss: 1.7458 - val_acc: 0.3750\n",
      "Epoch 19/50\n",
      " - 2s - loss: 1.0918 - acc: 0.5565 - val_loss: 1.5849 - val_acc: 0.4000\n",
      "Epoch 20/50\n",
      " - 2s - loss: 1.1672 - acc: 0.5357 - val_loss: 1.3688 - val_acc: 0.4750\n",
      "Epoch 21/50\n",
      " - 2s - loss: 1.1064 - acc: 0.5964 - val_loss: 1.6967 - val_acc: 0.3542\n",
      "Epoch 22/50\n",
      " - 2s - loss: 1.0379 - acc: 0.6094 - val_loss: 1.6558 - val_acc: 0.4250\n",
      "Epoch 23/50\n",
      " - 2s - loss: 1.1283 - acc: 0.6071 - val_loss: 1.9147 - val_acc: 0.3750\n",
      "Epoch 24/50\n",
      " - 1s - loss: 1.3558 - acc: 0.5685 - val_loss: 1.5308 - val_acc: 0.5250\n",
      "Epoch 25/50\n",
      " - 2s - loss: 1.0725 - acc: 0.5980 - val_loss: 1.6181 - val_acc: 0.3958\n",
      "Epoch 26/50\n",
      " - 2s - loss: 1.2955 - acc: 0.5733 - val_loss: 1.6274 - val_acc: 0.3000\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.9606 - acc: 0.6161 - val_loss: 1.4651 - val_acc: 0.2750\n",
      "Epoch 28/50\n",
      " - 2s - loss: 1.0609 - acc: 0.6430 - val_loss: 1.9117 - val_acc: 0.3000\n",
      "Epoch 29/50\n",
      " - 2s - loss: 1.1165 - acc: 0.5833 - val_loss: 1.4095 - val_acc: 0.5000\n",
      "Epoch 30/50\n",
      " - 2s - loss: 1.0151 - acc: 0.6250 - val_loss: 1.5010 - val_acc: 0.4000\n",
      "Epoch 31/50\n",
      " - 2s - loss: 1.2674 - acc: 0.6161 - val_loss: 1.7956 - val_acc: 0.2750\n",
      "Epoch 32/50\n",
      " - 1s - loss: 1.2743 - acc: 0.5192 - val_loss: 1.3701 - val_acc: 0.5250\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.9492 - acc: 0.6947 - val_loss: 1.4596 - val_acc: 0.4792\n",
      "Epoch 34/50\n",
      " - 2s - loss: 0.8264 - acc: 0.7308 - val_loss: 1.8206 - val_acc: 0.3500\n",
      "Epoch 35/50\n",
      " - 2s - loss: 1.0128 - acc: 0.6518 - val_loss: 2.4176 - val_acc: 0.2750\n",
      "Epoch 36/50\n",
      " - 2s - loss: 1.2420 - acc: 0.6159 - val_loss: 1.5016 - val_acc: 0.3500\n",
      "Epoch 37/50\n",
      " - 2s - loss: 0.9926 - acc: 0.6875 - val_loss: 2.3602 - val_acc: 0.1458\n",
      "Epoch 38/50\n",
      " - 2s - loss: 1.6686 - acc: 0.5195 - val_loss: 1.6935 - val_acc: 0.3000\n",
      "Epoch 39/50\n",
      " - 2s - loss: 0.9493 - acc: 0.6339 - val_loss: 1.4200 - val_acc: 0.4250\n",
      "Epoch 40/50\n",
      " - 2s - loss: 0.9726 - acc: 0.6352 - val_loss: 1.3601 - val_acc: 0.4000\n",
      "Epoch 41/50\n",
      " - 2s - loss: 0.8726 - acc: 0.6983 - val_loss: 1.3654 - val_acc: 0.5417\n",
      "Epoch 42/50\n",
      " - 2s - loss: 0.7230 - acc: 0.7500 - val_loss: 1.5431 - val_acc: 0.5000\n",
      "Epoch 43/50\n",
      " - 2s - loss: 0.9881 - acc: 0.6418 - val_loss: 1.7636 - val_acc: 0.3500\n",
      "Epoch 44/50\n",
      " - 2s - loss: 1.0803 - acc: 0.7161 - val_loss: 1.5541 - val_acc: 0.5000\n",
      "Epoch 45/50\n",
      " - 2s - loss: 1.0679 - acc: 0.6339 - val_loss: 2.0249 - val_acc: 0.3542\n",
      "Epoch 46/50\n",
      " - 2s - loss: 1.1262 - acc: 0.6442 - val_loss: 1.4649 - val_acc: 0.5250\n",
      "Epoch 47/50\n",
      " - 2s - loss: 0.7813 - acc: 0.7230 - val_loss: 1.2875 - val_acc: 0.5500\n",
      "Epoch 48/50\n",
      " - 1s - loss: 1.0427 - acc: 0.6689 - val_loss: 1.7874 - val_acc: 0.3250\n",
      "Epoch 49/50\n",
      " - 2s - loss: 1.0464 - acc: 0.6262 - val_loss: 1.4625 - val_acc: 0.4167\n",
      "Epoch 50/50\n",
      " - 2s - loss: 0.8817 - acc: 0.6875 - val_loss: 1.5153 - val_acc: 0.4500\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_flow, \n",
    "    steps_per_epoch = num_train_imgs//batch_size,\n",
    "    verbose = 2, \n",
    "    validation_data = valid_flow,   \n",
    "    validation_steps = num_valid_imgs//batch_size,\n",
    "    epochs=50)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 143s - loss: 1.9422 - acc: 0.2491 - val_loss: 1.9831 - val_acc: 0.1282\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 137s - loss: 1.8522 - acc: 0.2364 - val_loss: 1.8058 - val_acc: 0.4103\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 136s - loss: 1.6444 - acc: 0.3543 - val_loss: 1.8145 - val_acc: 0.2564\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 159s - loss: 1.8091 - acc: 0.3017 - val_loss: 2.0996 - val_acc: 0.1538\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 163s - loss: 1.7562 - acc: 0.4141 - val_loss: 1.7109 - val_acc: 0.4103\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 156s - loss: 1.7219 - acc: 0.3572 - val_loss: 1.9924 - val_acc: 0.2051\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 156s - loss: 1.5179 - acc: 0.4462 - val_loss: 1.6511 - val_acc: 0.3846\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 161s - loss: 1.6906 - acc: 0.3548 - val_loss: 1.6114 - val_acc: 0.4615\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n",
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n",
      " - 161s - loss: 1.5394 - acc: 0.4487 - val_loss: 1.4971 - val_acc: 0.4615\n",
      "old directory removed data/model_train\n",
      "old directory removed data/model_valid\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_train  validation:  data/model_valid\n",
      "137  images copied as training data\n",
      "39  images copied as validation data\n",
      "write csv file with training data: data/model_train.csv\n",
      "write csv file with validation data: data/model_valid.csv\n",
      "Found 137 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 images belonging to 7 classes.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b02b1038e73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_flow\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# to be used later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_valid_imgs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         epochs=1)              \n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'180129_InceptV3_20_Epochs_7_classes_CV.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2193\u001b[0m                                 \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m                                 \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m                                 workers=0)\n\u001b[0m\u001b[1;32m   2196\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                                      \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m                                      str(generator_output))\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train in cross validation loop\n",
    "for i in range(20):\n",
    "    # create new environment with new random train / valid split\n",
    "    num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = train_dir,\n",
    "       train_csv = train_csv,\n",
    "       valid_dir = valid_dir,\n",
    "       valid_csv = valid_csv,\n",
    "       train_valid = 0.7,\n",
    "       sub_dirs = True) \n",
    "\n",
    "    # here change hyperparameters.........\n",
    "    \n",
    "    # define image generator\n",
    "    train_gen = image.ImageDataGenerator(\n",
    "        # featurewise_center=True,\n",
    "        # featurewise_std_normalization=True,\n",
    "        rescale = 1./255,   # redundant with featurewise_center ? \n",
    "        # preprocessing_function=preprocess_input, not used in most examples\n",
    "        # horizontal_flip = True,    # no, as individual shapes are looked for\n",
    "        fill_mode = \"nearest\",\n",
    "        zoom_range = 0.3,\n",
    "        width_shift_range = 0.3,\n",
    "        height_shift_range=0.3,\n",
    "        rotation_range=30)\n",
    "\n",
    "    train_flow = train_gen.flow_from_directory(\n",
    "        train_dir,\n",
    "        # save_to_dir = \"data/model_train/augmented\",    \n",
    "        # color_mode = \"grayscale\",\n",
    "        target_size = (299,299),\n",
    "        batch_size = batch_size, \n",
    "        class_mode = \"categorical\")\n",
    "\n",
    "    valid_gen = image.ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        fill_mode = \"nearest\")\n",
    "\n",
    "    valid_flow = valid_gen.flow_from_directory(\n",
    "        valid_dir,\n",
    "        target_size = (299,299),\n",
    "        batch_size = batch_size,         \n",
    "        class_mode = \"categorical\") \n",
    "\n",
    "    hist = model.fit_generator(\n",
    "        train_flow, \n",
    "        steps_per_epoch = num_train_imgs//batch_size,\n",
    "        verbose = 2, \n",
    "        validation_data = valid_flow,   # to be used later\n",
    "        validation_steps = num_valid_imgs//batch_size,\n",
    "        epochs=10)              \n",
    "\n",
    "model.save('180129_InceptV3_20_Epochs_7_classes_CV_gaga.h5')\n",
    "\n",
    "# here the model started already from pretrained status (initial loss was ~8.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old directory removed data/model_test\n",
      "copy 34 images for whale # 1, called w_1287fbc\n",
      "copy 27 images for whale # 2, called w_98baff9\n",
      "copy 26 images for whale # 3, called w_7554f44\n",
      "copy 23 images for whale # 4, called w_1eafe46\n",
      "copy 22 images for whale # 5, called w_693c9ee\n",
      "copy 22 images for whale # 6, called w_ab4cae2\n",
      "copy 22 images for whale # 7, called w_fd1cb9d\n",
      "176  images of  7  whales copied in total\n",
      "Target Directory train:  data/model_test  validation:  None\n",
      "176  images copied as training data\n",
      "0  images copied as validation data\n",
      "write csv file with training data: data/model_test.csv\n"
     ]
    }
   ],
   "source": [
    "# training seems so work - accuracy of ~0.5 on training and validation data\n",
    "\n",
    "# try to verify on test data --> no success so far\n",
    "\n",
    "# use all training data of the first num_classes (7) whales a test data.\n",
    "# no good practice, but all training data have been augmented, so at least some indication\n",
    "# about predictive power of model\n",
    "test_dir = \"data/model_test\"\n",
    "test_csv = \"data/model_test.csv\"\n",
    "num_train_imgs, num_valid_imgs = create_small_case(\n",
    "       sel_whales = np.arange(1,num_classes+1),  # whales to be considered\n",
    "       all_train_dir = all_train_dir,\n",
    "       all_train_csv = all_train_csv,\n",
    "       train_dir = test_dir,\n",
    "       train_csv = test_csv,\n",
    "       valid_dir = None,     # no validation, copy all data into test_dir \"data/model_test\"\n",
    "       valid_csv = None,\n",
    "       train_valid = 1.,\n",
    "       sub_dirs = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 176 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# for test Purposes !!!\n",
    "\n",
    "# valid_gen = image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_gen = image.ImageDataGenerator(\n",
    "    # featurewise_center=True,\n",
    "    # featurewise_std_normalization=True,\n",
    "    rescale = 1./255,\n",
    "    # preprocessing_function=preprocess_input,   # model specific function\n",
    "    fill_mode = \"nearest\")\n",
    "\n",
    "test_flow = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    # color_mode = \"grayscale\",\n",
    "    batch_size = batch_size,     \n",
    "    target_size = (299,299),\n",
    "    class_mode = \"categorical\")    # use \"None\" ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 191s 17s/step\n",
      "(176, 7)\n",
      "[[  4.63210978e-03   3.00654769e-03   2.08380647e-04   3.28651840e-05\n",
      "    2.13375757e-03   2.42068578e-04   9.89744365e-01]\n",
      " [  4.22893502e-02   1.91685081e-01   1.02025822e-01   1.22443028e-02\n",
      "    2.54318833e-01   3.79027516e-01   1.84090622e-02]\n",
      " [  4.87957336e-02   3.56731176e-01   1.06019750e-01   2.23210137e-02\n",
      "    1.65488258e-01   2.62976795e-01   3.76672558e-02]\n",
      " [  1.11918822e-01   3.65958400e-02   1.60670914e-02   8.33930506e-04\n",
      "    6.80789471e-01   1.00539647e-01   5.32551333e-02]\n",
      " [  1.64128207e-02   3.37458588e-02   4.00195569e-02   4.13647760e-03\n",
      "    7.77367234e-01   1.02771342e-01   2.55466010e-02]\n",
      " [  6.20104000e-03   8.65606666e-01   3.90951335e-03   4.19794087e-04\n",
      "    9.49497707e-03   6.82832021e-03   1.07539691e-01]\n",
      " [  1.24325147e-02   1.17013408e-02   1.69343483e-02   9.31790099e-04\n",
      "    8.58521998e-01   9.30807367e-02   6.39727805e-03]\n",
      " [  2.70989574e-02   5.69240987e-01   7.86153376e-02   7.00570410e-03\n",
      "    1.14280954e-01   1.72118634e-01   3.16394903e-02]\n",
      " [  5.97229116e-02   3.86115313e-02   2.65900511e-02   3.94767337e-03\n",
      "    3.93650353e-01   2.77946275e-02   4.49682921e-01]\n",
      " [  1.75727624e-02   4.44667876e-01   3.92937250e-02   1.31709839e-03\n",
      "    3.76334459e-01   1.05533734e-01   1.52802188e-02]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_flow, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w_1eafe46': 1, 'w_7554f44': 3, 'w_fd1cb9d': 6, 'w_ab4cae2': 5, 'w_98baff9': 4, 'w_1287fbc': 0, 'w_693c9ee': 2}\n",
      "{0: 'w_1287fbc', 1: 'w_1eafe46', 2: 'w_693c9ee', 3: 'w_7554f44', 4: 'w_98baff9', 5: 'w_ab4cae2', 6: 'w_fd1cb9d'}\n"
     ]
    }
   ],
   "source": [
    "whale_class_map = (test_flow.class_indices)           # get dict mapping whalenames --> class_no\n",
    "class_whale_map = make_label_dict(directory=test_dir) # get dict mapping class_no --> whalenames\n",
    "print(whale_class_map)\n",
    "print(class_whale_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 7)\n",
      "[[  4.63210978e-03   3.00654769e-03   2.08380647e-04   3.28651840e-05\n",
      "    2.13375757e-03   2.42068578e-04   9.89744365e-01]\n",
      " [  4.22893502e-02   1.91685081e-01   1.02025822e-01   1.22443028e-02\n",
      "    2.54318833e-01   3.79027516e-01   1.84090622e-02]\n",
      " [  4.87957336e-02   3.56731176e-01   1.06019750e-01   2.23210137e-02\n",
      "    1.65488258e-01   2.62976795e-01   3.76672558e-02]\n",
      " [  1.11918822e-01   3.65958400e-02   1.60670914e-02   8.33930506e-04\n",
      "    6.80789471e-01   1.00539647e-01   5.32551333e-02]\n",
      " [  1.64128207e-02   3.37458588e-02   4.00195569e-02   4.13647760e-03\n",
      "    7.77367234e-01   1.02771342e-01   2.55466010e-02]\n",
      " [  6.20104000e-03   8.65606666e-01   3.90951335e-03   4.19794087e-04\n",
      "    9.49497707e-03   6.82832021e-03   1.07539691e-01]\n",
      " [  1.24325147e-02   1.17013408e-02   1.69343483e-02   9.31790099e-04\n",
      "    8.58521998e-01   9.30807367e-02   6.39727805e-03]\n",
      " [  2.70989574e-02   5.69240987e-01   7.86153376e-02   7.00570410e-03\n",
      "    1.14280954e-01   1.72118634e-01   3.16394903e-02]\n",
      " [  5.97229116e-02   3.86115313e-02   2.65900511e-02   3.94767337e-03\n",
      "    3.93650353e-01   2.77946275e-02   4.49682921e-01]\n",
      " [  1.75727624e-02   4.44667876e-01   3.92937250e-02   1.31709839e-03\n",
      "    3.76334459e-01   1.05533734e-01   1.52802188e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)\n",
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model predictions: \n",
      " [['w_fd1cb9d' 'w_1287fbc' 'w_1eafe46' 'w_98baff9' 'w_ab4cae2']\n",
      " ['w_ab4cae2' 'w_98baff9' 'w_1eafe46' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_1287fbc' 'w_ab4cae2' 'w_fd1cb9d' 'w_1eafe46']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1eafe46' 'w_fd1cb9d']\n",
      " ['w_1eafe46' 'w_fd1cb9d' 'w_98baff9' 'w_ab4cae2' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc' 'w_1eafe46']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_fd1cb9d']\n",
      " ['w_fd1cb9d' 'w_98baff9' 'w_1287fbc' 'w_1eafe46' 'w_ab4cae2']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_1eafe46' 'w_fd1cb9d' 'w_1287fbc']\n",
      " ['w_ab4cae2' 'w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_ab4cae2' 'w_1287fbc']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_fd1cb9d' 'w_693c9ee']\n",
      " ['w_7554f44' 'w_ab4cae2' 'w_1eafe46' 'w_693c9ee' 'w_98baff9']\n",
      " ['w_1eafe46' 'w_ab4cae2' 'w_98baff9' 'w_693c9ee' 'w_fd1cb9d']\n",
      " ['w_fd1cb9d' 'w_1eafe46' 'w_1287fbc' 'w_98baff9' 'w_693c9ee']\n",
      " ['w_1eafe46' 'w_98baff9' 'w_ab4cae2' 'w_693c9ee' 'w_1287fbc']\n",
      " ['w_ab4cae2' 'w_1eafe46' 'w_98baff9' 'w_693c9ee' 'w_7554f44']\n",
      " ['w_98baff9' 'w_ab4cae2' 'w_1eafe46' 'w_693c9ee' 'w_1287fbc']]\n",
      "true labels \n",
      " ['w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc' 'w_1287fbc'\n",
      " 'w_1287fbc' 'w_1287fbc']\n"
     ]
    }
   ],
   "source": [
    "# ge list of model predictions: one ordered list of maxpred whalenames per image\n",
    "top_k = preds.argsort()[:, -max_preds:][:, ::-1]\n",
    "# top_k = preds.argsort()[:, -max_preds:]\n",
    "model_preds = [([class_whale_map[i] for i in line]) for line in top_k]  \n",
    "\n",
    "# get list of true labels: one whalename per image\n",
    "# test_list = read_csv(file_name = test_csv)    # list with (filename, whalename)\n",
    "true_labels = []\n",
    "for fn in test_flow.filenames:\n",
    "    offset, filename = fn.split('/')\n",
    "    whale = [line[1] for line in test_list if line[0]==filename][0]\n",
    "    true_labels.append(whale)\n",
    "\n",
    "print(\"model predictions: \\n\", np.array(model_preds)[0:20])\n",
    "print(\"true labels \\n\", np.array(true_labels)[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP 0.321022727273\n",
      "Dummy MAP weighted 0.251325757576\n",
      "Dummy MAP weighted 0.311931818182\n",
      "Dummy MAP weighted 0.283049242424\n",
      "Dummy MAP weighted 0.324621212121\n",
      "Dummy MAP weighted 0.264962121212\n",
      "Dummy MAP weighted 0.273863636364\n",
      "Dummy MAP weighted 0.310700757576\n",
      "Dummy MAP weighted 0.261363636364\n",
      "Dummy MAP weighted 0.317140151515\n",
      "Dummy MAP weighted 0.289583333333\n"
     ]
    }
   ],
   "source": [
    "MAP = mean_average_precision(model_preds, true_labels, max_preds)\n",
    "print(\"MAP\", MAP)\n",
    "\n",
    "for i in range(10):\n",
    "    Dummy_map = Dummy_MAP(probs = 'weighted', distributed_as = train_csv, image_no = len(test_list))\n",
    "    print(\"Dummy MAP weighted\", Dummy_map)\n",
    "\n",
    "# MAP only slightly higher than averag dummy MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
