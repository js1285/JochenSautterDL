{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.core.debugger import set_trace\n",
    "import time\n",
    "\n",
    "# import _pickle as cPickle\n",
    "# import os\n",
    "# import gzip\n",
    "# from IPython.core.debugger import set_trace\n",
    "# import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise: CNN\n",
    "In this exercise you will implement a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "mnist Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f027264b860>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f0264e4c320>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f0264e4cb38>)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# set_trace()\n",
    "print(\"mnist {}\".format(mnist))\n",
    "# set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_graph(filter_no = 16):\n",
    "\n",
    "    # define input variable\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "    \n",
    "    # initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients\n",
    "    # The generated values follow a normal distribution with specified mean and standard deviation, \n",
    "    # except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n",
    "    def weight_variable(shape):\n",
    "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    # define small positive constant bias\n",
    "    def bias_variable(shape):\n",
    "      initial = tf.constant(0.1, shape=shape)\n",
    "      return tf.Variable(initial)\n",
    "\n",
    "    # Computes a 2-D convolution given 4-D input and filter tensors. returning 4D tensor of same type as x\n",
    "    # ? means zero padding Ã¼berall 2, oder ?\n",
    "    # X has 4D data format [batch, height, width, color channels]\n",
    "    # filter w has shape [filter_height, filter_width, in_channels, out_channels]  (no of filters, first colors...?)\n",
    "    # strides = The stride of the sliding window for each dimension of input \n",
    "    def conv2d(x, W):\n",
    "      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def max_pool_2x2(x):\n",
    "      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# def define_graph(filter_no = 16):\n",
    "\n",
    "    # first convolutional layer with 16 filters, one color\n",
    "    # initialise weights and bias (slightly randomized)\n",
    "    # 3 x 3, 1 input (only one color), 16 output channels\n",
    "    W_conv1 = weight_variable([3, 3, 1, filter_no])\n",
    "    b_conv1 = bias_variable([filter_no])\n",
    "\n",
    "    # define tensor representing input layer: reshape flattened 748 Vector to 4d tensor 28 x 28 image matrices\n",
    "    # and the final dimension corresponding to the number of color channels.\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # convolve, same size (padding, stride...), reduce size to 14x14 by maxpooling\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # define second layer convolving same size, \n",
    "    W_conv2 = weight_variable([3, 3, filter_no, filter_no])  # ??\n",
    "    b_conv2 = bias_variable([filter_no])\n",
    "\n",
    "    # convolve, same size (padding, stride...), reduce size to 7x7 by maxpooling\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # we add a fully-connected layer with 128 neurons to allow processing on the entire image.\n",
    "    # We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, \n",
    "    # and apply a ReLU.\n",
    "    W_fc1 = weight_variable([7 * 7 * filter_no, 128])\n",
    "    b_fc1 = bias_variable([128])\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*filter_no])   # shape is inferred as to fit to second argument\n",
    "    #test1 = tf.shape(h_pool2_flat)\n",
    "    #print(\"test1: {}\".format(test1))\n",
    "    #test2 = h_pool2_flat.get_shape()\n",
    "    #print(\"test2: {}\".format(test2))\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # implement dropout\n",
    "    # keep_prob = tf.placeholder(tf.float32)\n",
    "    # h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # final fully connected layer\n",
    "    W_fc2 = weight_variable([128, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "\n",
    "    # y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "\n",
    "    return (x, y_conv, y_)\n",
    "\n",
    "def count_number_trainable_params():\n",
    "    # Counts the number of trainable variables.\n",
    "    tot_nb_params = 0\n",
    "    for trainable_variable in tf.trainable_variables():\n",
    "        shape = trainable_variable.get_shape() # e.g [D,F] or [W,H,C]\n",
    "        current_nb_params = get_nb_params_shape(shape)\n",
    "        tot_nb_params = tot_nb_params + current_nb_params\n",
    "    return tot_nb_params\n",
    "\n",
    "def get_nb_params_shape(shape):\n",
    "    # Computes the total number of params for a given shape.\n",
    "    # Works for any number of shapes etc [D,F] or [W,H,C] computes D*F and W*H*C.\n",
    "    nb_params = 1\n",
    "    for dim in shape:\n",
    "        nb_params = nb_params*int(dim)\n",
    "    return nb_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_learning_rates(x, y_conv, y_):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "    # train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    epochs = 10\n",
    "    learning_rates = np.array([0.1,0.01,0.001,0.0001])\n",
    "    test_accuracies = np.zeros([learning_rates.shape[0],epochs])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        for k, learning_rate in enumerate(learning_rates):\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(100):\n",
    "                    batch = mnist.train.next_batch(50)\n",
    "                    if i % 50 == 0:\n",
    "                        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "                        print(\"step {}, batch training accuracy {:.4f}\".format(i, train_accuracy))\n",
    "                    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "                test_accuracy = accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                test_accuracies[k,epoch] = test_accuracy\n",
    "                print(\"test accuracy epoch {}: {:.4f}\".format(epoch, test_accuracy))\n",
    "            print(\"learningrate {} finished\\n\".format(learning_rate))\n",
    "    # print(\"test_accuracies {}\".format(test_accuracies))\n",
    "    return(learning_rates, test_accuracies)\n",
    "\n",
    "def execute_filters(x, y_conv, y_):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "    # train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    epochs = 4\n",
    "    learning_rate = 0.1\n",
    "    # learning_rates = np.array([0.1,0.01,0.001,0.0001])\n",
    "    # test_accuracies = np.zeros([learning_rates.shape[0],epochs])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(50):\n",
    "                batch = mnist.train.next_batch(50)\n",
    "                if i % 50 == 0:\n",
    "                    train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "                    print(\"step {}, batch training accuracy {:.4f}\".format(i, train_accuracy))\n",
    "                train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "            test_accuracy = accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "            # test_accuracies[k,epoch] = test_accuracy\n",
    "            print(\"test accuracy epoch {}: {:.4f}\".format(epoch, test_accuracy))\n",
    "    # print(\"test_accuracies {}\".format(test_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, batch training accuracy 0.1000\n",
      "step 50, batch training accuracy 0.6800\n",
      "test accuracy epoch 0: 0.8102\n",
      "step 0, batch training accuracy 0.7600\n",
      "step 50, batch training accuracy 0.8600\n",
      "test accuracy epoch 1: 0.9315\n",
      "step 0, batch training accuracy 0.9400\n",
      "step 50, batch training accuracy 0.9400\n",
      "test accuracy epoch 2: 0.9359\n",
      "step 0, batch training accuracy 0.9200\n",
      "step 50, batch training accuracy 0.9600\n",
      "test accuracy epoch 3: 0.9441\n",
      "step 0, batch training accuracy 1.0000\n",
      "step 50, batch training accuracy 0.9400\n",
      "test accuracy epoch 4: 0.9516\n",
      "step 0, batch training accuracy 1.0000\n",
      "step 50, batch training accuracy 0.9800\n",
      "test accuracy epoch 5: 0.9663\n",
      "step 0, batch training accuracy 1.0000\n",
      "step 50, batch training accuracy 0.9200\n",
      "test accuracy epoch 6: 0.9618\n",
      "step 0, batch training accuracy 0.9600\n",
      "step 50, batch training accuracy 0.9400\n",
      "test accuracy epoch 7: 0.9643\n",
      "step 0, batch training accuracy 0.9800\n",
      "step 50, batch training accuracy 0.9600\n",
      "test accuracy epoch 8: 0.9732\n",
      "step 0, batch training accuracy 1.0000\n",
      "step 50, batch training accuracy 0.9800\n",
      "test accuracy epoch 9: 0.9728\n",
      "learningrate 0.1 finished\n",
      "\n",
      "step 0, batch training accuracy 0.1800\n",
      "step 50, batch training accuracy 0.2800\n",
      "test accuracy epoch 0: 0.4133\n",
      "step 0, batch training accuracy 0.3600\n",
      "step 50, batch training accuracy 0.4200\n",
      "test accuracy epoch 1: 0.7122\n",
      "step 0, batch training accuracy 0.6600\n",
      "step 50, batch training accuracy 0.8200\n",
      "test accuracy epoch 2: 0.7955\n",
      "step 0, batch training accuracy 0.7200\n",
      "step 50, batch training accuracy 0.7600\n",
      "test accuracy epoch 3: 0.8662\n",
      "step 0, batch training accuracy 0.8800\n",
      "step 50, batch training accuracy 0.8600\n",
      "test accuracy epoch 4: 0.8935\n",
      "step 0, batch training accuracy 0.9600\n",
      "step 50, batch training accuracy 0.9400\n",
      "test accuracy epoch 5: 0.9052\n",
      "step 0, batch training accuracy 0.9000\n",
      "step 50, batch training accuracy 0.8800\n",
      "test accuracy epoch 6: 0.9077\n",
      "step 0, batch training accuracy 0.9400\n",
      "step 50, batch training accuracy 0.9400\n",
      "test accuracy epoch 7: 0.9167\n",
      "step 0, batch training accuracy 0.9000\n",
      "step 50, batch training accuracy 0.9800\n",
      "test accuracy epoch 8: 0.9166\n",
      "step 0, batch training accuracy 0.9000\n",
      "step 50, batch training accuracy 0.9600\n",
      "test accuracy epoch 9: 0.9288\n",
      "learningrate 0.01 finished\n",
      "\n",
      "step 0, batch training accuracy 0.0600\n",
      "step 50, batch training accuracy 0.1200\n",
      "test accuracy epoch 0: 0.1017\n",
      "step 0, batch training accuracy 0.1000\n",
      "step 50, batch training accuracy 0.1400\n",
      "test accuracy epoch 1: 0.1397\n",
      "step 0, batch training accuracy 0.0800\n",
      "step 50, batch training accuracy 0.1400\n",
      "test accuracy epoch 2: 0.1763\n",
      "step 0, batch training accuracy 0.2600\n",
      "step 50, batch training accuracy 0.1600\n",
      "test accuracy epoch 3: 0.1995\n",
      "step 0, batch training accuracy 0.2400\n",
      "step 50, batch training accuracy 0.1400\n",
      "test accuracy epoch 4: 0.2264\n",
      "step 0, batch training accuracy 0.2400\n",
      "step 50, batch training accuracy 0.2800\n",
      "test accuracy epoch 5: 0.2789\n",
      "step 0, batch training accuracy 0.2800\n",
      "step 50, batch training accuracy 0.3000\n",
      "test accuracy epoch 6: 0.3203\n",
      "step 0, batch training accuracy 0.2800\n",
      "step 50, batch training accuracy 0.3800\n",
      "test accuracy epoch 7: 0.3638\n",
      "step 0, batch training accuracy 0.3600\n",
      "step 50, batch training accuracy 0.3600\n",
      "test accuracy epoch 8: 0.3946\n",
      "step 0, batch training accuracy 0.4000\n",
      "step 50, batch training accuracy 0.4000\n",
      "test accuracy epoch 9: 0.4332\n",
      "learningrate 0.001 finished\n",
      "\n",
      "step 0, batch training accuracy 0.0800\n",
      "step 50, batch training accuracy 0.1200\n",
      "test accuracy epoch 0: 0.0978\n",
      "step 0, batch training accuracy 0.1000\n",
      "step 50, batch training accuracy 0.0600\n",
      "test accuracy epoch 1: 0.0972\n",
      "step 0, batch training accuracy 0.1600\n",
      "step 50, batch training accuracy 0.1400\n",
      "test accuracy epoch 2: 0.0968\n",
      "step 0, batch training accuracy 0.0600\n",
      "step 50, batch training accuracy 0.1600\n",
      "test accuracy epoch 3: 0.0985\n",
      "step 0, batch training accuracy 0.0400\n",
      "step 50, batch training accuracy 0.1200\n",
      "test accuracy epoch 4: 0.1014\n",
      "step 0, batch training accuracy 0.0200\n",
      "step 50, batch training accuracy 0.1600\n",
      "test accuracy epoch 5: 0.1045\n",
      "step 0, batch training accuracy 0.0800\n",
      "step 50, batch training accuracy 0.1400\n",
      "test accuracy epoch 6: 0.1060\n",
      "step 0, batch training accuracy 0.0400\n",
      "step 50, batch training accuracy 0.0600\n",
      "test accuracy epoch 7: 0.1092\n",
      "step 0, batch training accuracy 0.0400\n",
      "step 50, batch training accuracy 0.0800\n",
      "test accuracy epoch 8: 0.1124\n",
      "step 0, batch training accuracy 0.0800\n",
      "step 50, batch training accuracy 0.1000\n",
      "test accuracy epoch 9: 0.1145\n",
      "learningrate 0.0001 finished\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y_conv, y_ = define_graph(filter_no = 16)\n",
    "learning_rates, test_accuracies = execute_learning_rates(x, y_conv, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e968c53065a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"learning rate: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epochs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for k, learning_rate in enumerate(learning_rates):\n",
    "    plt.plot(test_accuracies[k,:], label=\"learning rate: {}\".format(learning_rate))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, batch training accuracy 0.0800\n",
      "test accuracy epoch 0: 0.5127\n",
      "step 0, batch training accuracy 0.5200\n",
      "test accuracy epoch 1: 0.8305\n",
      "step 0, batch training accuracy 0.8200\n",
      "test accuracy epoch 2: 0.8580\n",
      "step 0, batch training accuracy 0.8600\n",
      "test accuracy epoch 3: 0.9168\n",
      "tests with 4 filters finished \n",
      "\n",
      "step 0, batch training accuracy 0.1000\n",
      "test accuracy epoch 0: 0.6590\n",
      "step 0, batch training accuracy 0.6400\n",
      "test accuracy epoch 1: 0.8027\n",
      "step 0, batch training accuracy 0.7800\n",
      "test accuracy epoch 2: 0.8417\n",
      "step 0, batch training accuracy 0.9000\n",
      "test accuracy epoch 3: 0.8974\n",
      "tests with 8 filters finished \n",
      "\n",
      "step 0, batch training accuracy 0.1000\n",
      "test accuracy epoch 0: 0.7834\n",
      "step 0, batch training accuracy 0.7200\n",
      "test accuracy epoch 1: 0.8861\n",
      "step 0, batch training accuracy 0.9400\n",
      "test accuracy epoch 2: 0.9225\n",
      "step 0, batch training accuracy 0.9400\n",
      "test accuracy epoch 3: 0.9097\n",
      "tests with 12 filters finished \n",
      "\n",
      "step 0, batch training accuracy 0.1000\n",
      "test accuracy epoch 0: 0.6537\n",
      "step 0, batch training accuracy 0.5800\n",
      "test accuracy epoch 1: 0.8980\n",
      "step 0, batch training accuracy 0.9200\n",
      "test accuracy epoch 2: 0.9081\n",
      "step 0, batch training accuracy 0.8400\n",
      "test accuracy epoch 3: 0.9409\n",
      "tests with 16 filters finished \n",
      "\n",
      "step 0, batch training accuracy 0.1200\n",
      "test accuracy epoch 0: 0.7597\n",
      "step 0, batch training accuracy 0.8000\n",
      "test accuracy epoch 1: 0.8988\n",
      "step 0, batch training accuracy 0.9400\n",
      "test accuracy epoch 2: 0.9360\n",
      "step 0, batch training accuracy 0.9200\n",
      "test accuracy epoch 3: 0.8674\n",
      "tests with 24 filters finished \n",
      "\n",
      "step 0, batch training accuracy 0.0600\n",
      "test accuracy epoch 0: 0.8197\n",
      "step 0, batch training accuracy 0.9200\n",
      "test accuracy epoch 1: 0.9036\n",
      "step 0, batch training accuracy 0.9400\n",
      "test accuracy epoch 2: 0.9214\n",
      "step 0, batch training accuracy 0.9000\n",
      "test accuracy epoch 3: 0.8786\n",
      "tests with 32 filters finished \n",
      "\n",
      "runtimes: [ 12.31616449  15.82876587  19.89440036  21.2884388   30.48604178\n",
      "  48.96675205]\n",
      "param_nos: [ 26694  52258  78110 104250 157394 211690]\n"
     ]
    }
   ],
   "source": [
    "# filters = (8,16,32,64,128,256)\n",
    "filters = (4,8,12,16,24,32)\n",
    "param_nos = np.zeros_like(filters)\n",
    "runtimes = np.zeros_like(filters,dtype=float)\n",
    "for i, filter in enumerate(filters):\n",
    "    t0 = time.time()\n",
    "    tf.reset_default_graph()   # without reset, new nodes are added to previous ones\n",
    "    x, y_conv, y_ = define_graph(filter_no = filter)\n",
    "    execute_filters(x, y_conv, y_)    # train and test with updated graph\n",
    "    runtimes[i] = time.time() - t0\n",
    "    param_nos[i] = count_number_trainable_params()\n",
    "    print(\"tests with {} filters finished \\n\".format(filter))\n",
    "\n",
    "print(\"runtimes: {}\".format(runtimes))\n",
    "print(\"param_nos: {}\".format(param_nos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'runtime / seconds')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAHjCAYAAADlk0M8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f025f18a588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.scatter(param_nos, runtimes)\n",
    "\n",
    "# for k, learning_rate in enumerate(param_nos):\n",
    "#     plt.plot(test_accuracies[k,:], label=\"learning rate: {}\".format(learning_rate))\n",
    "plt.grid(True)\n",
    "plt.xlim(0)\n",
    "plt.ylim(0)\n",
    "plt.xlabel(\"total number of variable parameters\")\n",
    "plt.ylabel(\"runtime / seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
