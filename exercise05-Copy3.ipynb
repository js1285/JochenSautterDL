{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth exercise (Chapter 7)Â¶\n",
    "\n",
    "Chapter 7 is about regularization, in this assignment we will implement various forms of regularization for MLP's. \n",
    "\n",
    "* L2 regularization\n",
    "* Dropout\n",
    "* Early stopping\n",
    "\n",
    "In the following code block, all necessary imports are provided as well as the data generators. You can use your MLP implementation from the previous assignment or the reference implementation attached. In this reference implementations, there are some slight differences with the network that was provided for the previous example, most notably: \n",
    "\n",
    "* The cost function now also has an update function integrated\n",
    "* The data format is standardized for all sets (train, test and validation)\n",
    "\n",
    "## Data\n",
    "\n",
    "We will make use of the digits dataset, standard packed in sklearn. It contains a similar classification task as MNIST, however the images are smaller (64 pixels), making experimenting with it faster. \n",
    "\n",
    "For your convenience, a convert function has been provided to convert the dataset into the format that the reference network accepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# import scipy.stats\n",
    "import random\n",
    "from sklearn.datasets import load_digits\n",
    "from IPython.core.debugger import set_trace\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "testsize = len(X) / 10\n",
    "validsize = len(X) / 10\n",
    "\n",
    "train_indices = np.arange(len(X) - testsize - validsize, dtype=np.int)\n",
    "valid_indices = np.arange(len(train_indices), len(X) - testsize, dtype=np.int)\n",
    "test_indices = np.arange(len(train_indices) + len(valid_indices), len(X), dtype=np.int)\n",
    "\n",
    "train_set = [(np.reshape(X[idx], (len(X[idx]), 1)), vectorized_result(y[idx])) for idx in train_indices]\n",
    "valid_set = [(np.reshape(X[idx], (len(X[idx]), 1)), vectorized_result(y[idx])) for idx in valid_indices]\n",
    "test_set  = [(np.reshape(X[idx], (len(X[idx]), 1)), vectorized_result(y[idx])) for idx in test_indices]\n",
    "\n",
    "num_features = len(X[0])\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Network\n",
    "\n",
    "Replace the cell below with your own network from exercise 6. Make sure that it doesn't deviate to much from the reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def Generate_DropOut_Mask(p_DropOut, sizes):   # p_DropOut is Tuple (p_input, p_hidden)\n",
    "    # Create Mask (List of arrays) for randomly switch on/off nodes - dim = sizes of Network as defined in Network init\n",
    "    mu = [] \n",
    "    for i, size in enumerate(sizes):     # sum of squared of each single weight parameter --> iterate through layers\n",
    "        if p_DropOut != None:\n",
    "            if i == 0:                   # DropOut[0] if input layer (i==0), \n",
    "                mask = bernoulli.rvs(p_DropOut[0], size=size)  \n",
    "            elif i < len(sizes):         # all hidden layers p_DropOut(1) = p_hidden \n",
    "                mask = bernoulli.rvs(p_DropOut[1], size=size)\n",
    "            else:\n",
    "                mask = np.ones(size, dtype=int)    # output layer always one --> no dropout\n",
    "            # set_trace()\n",
    "        else: \n",
    "            mask = np.ones(size, dtype=int) # all one, mask has no effect\n",
    "        mu.append(mask)  \n",
    "    set_trace()\n",
    "    return mu\n",
    "\n",
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y, weights = []):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \n",
    "        a is an array (size (num_classes, 1)) the activations of the output_layer,\n",
    "        y is an array (same size) with the class label.\n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the derivative of the cost function wrt z.\n",
    "           Recall that a=sigmoid(z).\n",
    "           \n",
    "           z, a and y are arrays of shape (num_classes, 1)\n",
    "        \"\"\"\n",
    "        return (a - y) * sigmoid_prime(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def update(w, nw, eta, size):\n",
    "        \"\"\"Does a single gradient step. \n",
    "        \n",
    "        w and nw are arrays with size corresponding to the weights vector; \n",
    "        eta and size are scalars. \n",
    "        \"\"\"\n",
    "        return w - (eta / size) * nw\n",
    "\n",
    "\n",
    "class RidgeCost(object):\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    # @staticmethod\n",
    "    def fn(self, a, y, weights):\n",
    "\n",
    "        squared_weights = 0\n",
    "        for w in weights:      # sum of squared of each single weight parameter --> iterate through layers\n",
    "            squared_weights += np.sum(w**2)   # sum of elementwise squared matrices\n",
    "\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2 + (self.alpha/2) * squared_weights\n",
    "\n",
    "    def delta(self, z, a, y):\n",
    "        \"\"\"Return the derivative of the cost function wrt z.\n",
    "           Recall that a=sigmoid(z).\n",
    "           \n",
    "           z, a and y are arrays of shape (num_classes, 1)\n",
    "        \"\"\"\n",
    "        return (a - y) * sigmoid_prime(z)  \n",
    "\n",
    "    # @staticmethod\n",
    "    def update(self, w, nw, eta, size):\n",
    "        \"\"\"Does a single gradient step. \n",
    "        \n",
    "        w and nw are arrays with size corresponding to the weights vector; \n",
    "        eta and size are scalars. \n",
    "        \"\"\"\n",
    "        return (1 - eta*self.alpha) * w - (eta / size) * nw\n",
    "    \n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=QuadraticCost, p_DropOut=None):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.cost = cost\n",
    "        self.p_DropOut = p_DropOut # Tuple (p_input, p_hidden)\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        # here only inference, not training - therefor DropOut mask not applied, but weight rescaling done instead\n",
    "        # print(\"weights before rescale {} \\n\".format(self.weights[1]))\n",
    "        for b, wi in zip(self.biases, self.weights):\n",
    "            if self.p_DropOut != None:      # rescale weights according to \"weight scaling inference rule\"\n",
    "                # set_trace()\n",
    "                w_next = wi * self.p_DropOut[1]   # p_hidden\n",
    "                # set_trace()\n",
    "            else:\n",
    "                w_next = wi\n",
    "            a = sigmoid(np.dot(w_next, a) + b)\n",
    "        set_trace()\n",
    "        return a\n",
    "\n",
    "#    def Rescale_Weights_DropOut():\n",
    "#        if self.p_DropOut != None\n",
    "#            self.weights = [w * self.p_DropOut[i!=0] for i, w in self.weights]\n",
    "#            # for i, w in enumerate(self.weights):\n",
    "#            #    self.weights = self.weights * self.p_DropOut[i!=0]  # DropOut[0] if input layer (i==0), DropOut(1) else\n",
    " \n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            validation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs. Here, x represents an array with dimensions \n",
    "        (num_attributes, 1); y is an array of size (num_classes, 1) \n",
    "        The other non-optional parameters are self-explanatory.  \n",
    "        If ``test_data`` is provided (same format as training_data) \n",
    "        then the network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "\n",
    "        if validation_data: n_data = len(validation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            if j % 10 == 0:\n",
    "                \n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "                if monitor_training_cost:\n",
    "                    cost = self.total_cost(training_data)\n",
    "                    training_cost.append(cost)\n",
    "                    print(\"Cost on training data: {}\".format(cost))\n",
    "                if monitor_training_accuracy:\n",
    "                    accuracy = self.accuracy(training_data)\n",
    "                    training_accuracy.append(accuracy)\n",
    "                    print(\"Accuracy on training data: {} / {}\".format(\n",
    "                        accuracy, n))\n",
    "                if monitor_evaluation_cost:\n",
    "                    cost = self.total_cost(validation_data)\n",
    "                    evaluation_cost.append(cost)\n",
    "                    print(\"Cost on evaluation data: {}\".format(cost))\n",
    "                if monitor_evaluation_accuracy:\n",
    "                    accuracy = self.accuracy(validation_data)\n",
    "                    evaluation_accuracy.append(accuracy)\n",
    "                    print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                        self.accuracy(validation_data), n_data))\n",
    "                    \n",
    "        return training_cost, training_accuracy, evaluation_cost, evaluation_accuracy     \n",
    "                    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)`` (similar to \n",
    "        the data structure defined in SGD) and ``eta`` is the learning \n",
    "        rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]   # initialise with zero\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        mu = Generate_DropOut_Mask(self.p_DropOut, self.sizes) # use one DropOut mask per mini batch\n",
    "        for x, y in mini_batch:\n",
    "            # set_trace()\n",
    "            # x = np.squeeze(x) * mu[0]    # elementwise (feature-wise) apply zero / one mask on sample\n",
    "            # set_trace()\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y, mu)    # delta_nabla_w are gradients\n",
    "            # sum up / accumulate nabla_bs of all samples in Mini-Batch\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]   \n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # perform update step with accumulated gradients (mean is taken inside update function)\n",
    "        self.weights = [self.cost.update(w, nw, eta, len(mini_batch)) for w, nw in zip(self.weights, nabla_w)]\n",
    "        # apply DropOut Mask to \n",
    "        # if self.p_DropOut != None:\n",
    "            # mu = Generate_DropOut_Mask(self.p_DropOut, self.sizes)\n",
    "            # print(\"mu input {} \\n\".format(mu[0]))\n",
    "            # print(\"mu first layer {} \\n\".format(mu[1]))\n",
    "\n",
    "            # print(\"weights input layer before DropOut {} \\n\".format(self.weights[0]))\n",
    "            # print(\"weights first layer before DropOut {} \\n\".format(self.weights[1]))    \n",
    "            # set_trace()\n",
    "            # self.weights = [w * mask for w,mask in zip(self.weights, mu)]\n",
    "            # print(\"weights input layer after DropOut {} \\n\".format(self.weights[0]))\n",
    "            # print(\"weights first layer after DropOut {} \\n\".format(self.weights[1]))            \n",
    "        self.biases = [self.cost.update(b, nb, eta, len(mini_batch)) for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y, mu):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward for training: here apply DropOut mask\n",
    "        activation = x\n",
    "        if self.p_DropOut != None:\n",
    "            set_trace()\n",
    "            activation[:,0] = activation[:,0] * mu[0]   # apply mu on input layer\n",
    "            set_trace()\n",
    "        # activation[:,0] = \n",
    "        activations = [activation]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer (A in Boedecker notation)\n",
    "        for b, w, m in zip(self.biases, self.weights, mu[1:]):  # run through mu list from second layer (= first hidden layer, mu[0] is first one is for input X)\n",
    "            set_trace()\n",
    "            z = np.dot(w, activation) + b    \n",
    "            # set_trace()\n",
    "            zs.append(z)\n",
    "            # set_trace()\n",
    "            activation = sigmoid(z)\n",
    "            if self.p_DropOut != None:\n",
    "                layer_mask = m.astype(np.float32)\n",
    "                activation[:,0] = activation[:,0] * layer_mask            \n",
    "            activations.append(activation)\n",
    "        set_trace()\n",
    "        # backward pass                                       # activations are Z in Boed- notation\n",
    "        delta = self.cost.delta(zs[-1], activations[-1], y)   # dL/dz = h'  (dL/dA in Boedecker notation)\n",
    "        delta[:,0] = delta[:,0] * mu[-1]                      # does nothing, as ones are stored in last layer of mask\n",
    "        nabla_b[-1] = delta                                   # dL/db = dL/dz   \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())   # dL/dw = A dot dL/dZ \n",
    "        # Note that the variable l in the loop below is used in the following way:\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]      # a1 bei Boedecker\n",
    "            sp = sigmoid_prime(z)    # h'(a1)\n",
    "            # start with (first loop, counting from l=2) -l+1 = -2+1 = -1 = last layer downwards until first hidden\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp  # Boed dL/dA0 = (dL/dA1 x w1).T * h'(A)\n",
    "            # set_trace()\n",
    "            # counting backwards, mask mu is indexed in same manner as weights w (see below for nabla[-l])\n",
    "            layer_mask = mu[-l].astype(np.float32)\n",
    "            set_trace()\n",
    "            delta[:,0] = delta[:,0] * layer_mask    # ?? necessary? should become zero automatically by \n",
    "            set_trace()\n",
    "            nabla_b[-l] = delta                                           # Boed dL/db = dL/dA\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())  # Boed dL/dW = Z0.T x dL/dA0\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.\n",
    "        \"\"\"\n",
    "        results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)   # a = y_pred\n",
    "            cost += self.cost.fn(a, y, self.weights) / len(data)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "Implement L2 regularization using the equations 7.1 - 7.5, by creating a new cost class (containing hyperparameter alpha), containing the same functions as QuadraticCost, i.e., fn, delta and update. Plot how test performance varies for various values of alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cc07e2246737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mmonitor_evaluation_accuracy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mmonitor_training_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 monitor_training_accuracy=True)   # orig epochs 300\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0me_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"alpha = {} evaluated\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-796052967c8f>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, validation_data, monitor_evaluation_cost, monitor_evaluation_accuracy, monitor_training_cost, monitor_training_accuracy)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmonitor_training_cost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                     \u001b[0mtraining_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cost on training data: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-796052967c8f>\u001b[0m in \u001b[0;36mtotal_cost\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m             \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-796052967c8f>\u001b[0m in \u001b[0;36mfeedforward\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_rs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "layers = [num_features, 50, 50, 50, num_classes]\n",
    "e_accuracies=[]\n",
    "alphas = [0.00,0.0001,0.001,0.01,0.1,1.0]\n",
    "# net = Network(layers, cost=r_cost)\n",
    "for alpha in alphas:\n",
    "    net = Network(layers, cost=RidgeCost(alpha = alpha))\n",
    "    t_cost, t_accuracy, e_cost, e_accuracy = net.SGD(train_set, 300, 50, 0.1, validation_data=test_set,\n",
    "                monitor_evaluation_cost=True,\n",
    "                monitor_evaluation_accuracy=True,\n",
    "                monitor_training_cost=True,\n",
    "                monitor_training_accuracy=True)   # orig epochs 300\n",
    "    e_accuracies.append(e_accuracy)\n",
    "    print(\"alpha = {} evaluated\".format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8b0841e2963b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_accuracy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# print(e_accuracy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training accuracy alpha {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e_accuracies' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe45c8e1630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for i, e_accuracy in enumerate(e_accuracies):\n",
    "    # print(e_accuracy)\n",
    "    plt.plot(e_accuracy, label=\"Training accuracy alpha {}\".format(alphas[i]))\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Implement dropout as explained in 7.12. Create a function generateMask(p_input, p_hidden) that generates a list of l arrays, each element in l being an array of size (layer_size, 1). Each element of the array on the first level is 1 with probability p_input. Each element of the array on the last level (output layer) is 1. Each element of the other arrays is 1 with probability p_hidden. For this you can use the distribution scipy.stats.bernoulli. Add hyperparameters p_input and p_hidden to the network. \n",
    "\n",
    "Inference occurs as presented in equation 7.52. Plot how the test performance varies for various values of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-23-ff46c35b9239>\u001b[0m(26)\u001b[0;36mGenerate_DropOut_Mask\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     24 \u001b[0;31m        \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     25 \u001b[0;31m    \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 26 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     28 \u001b[0;31m\u001b[0;32mclass\u001b[0m \u001b[0mQuadraticCost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "layers = [num_features, 50, 50, 50, num_classes]\n",
    "net = Network(layers, cost=QuadraticCost, p_DropOut = [0.8, 0.5])   # input / hidden unit\n",
    "# net = Network(layers, p_DropOut = None)   # input / hidden unit\n",
    "t_cost, t_accuracy, e_cost, e_accuracy = net.SGD(train_set, 100, 50, 0.1, validation_data=test_set,\n",
    "                monitor_evaluation_cost=True,\n",
    "                monitor_evaluation_accuracy=True,\n",
    "                monitor_training_cost=True,\n",
    "                monitor_training_accuracy=True)   # orig epochs 300   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Implement early stopping as presented in algorithm 7.1 and 7.2. Make sure to make proper use of the validation and the test set (as provided above). \n",
    "\n",
    "Demonstrate the correct usage by plotting learning curves for train, validation and test performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
