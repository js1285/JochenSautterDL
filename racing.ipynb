{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    racing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tools as t\n",
    "import models as m\n",
    "import json\n",
    "import ijson\n",
    "\n",
    "# import hyperband as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now write new file  data/engadin_2000.json\n"
     ]
    }
   ],
   "source": [
    "gaga = t.write_clean_jason('data/engadiner-2000-neu.json', 'data/engadin_2000.json')\n",
    "# gaga = t.write_clean_jason('data/engadiner-2000-neu.json', 'data/engadin_2000_mini_2000.json', from_to=(0,2000))\n",
    "# t.write_clean_jason('data/enrichedMeasurements.json', 'data/engadin_5000_mini_all.json', from_to=(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len runner_list 5\n",
      "runner_list [431, 2441, 3154, 5612, 8530]\n",
      "avg_dist 0.00138031591663\n",
      "avg_dh 0.0253626862556\n",
      "avg_dist_var 0.00357118631224\n",
      "avg_dh_var 0.00015964166629\n",
      "avg_dist_stdev 0.0597594035465\n",
      "avg_dh_stdev 0.0126349383176\n",
      "padding segments with zeros up to length of:  75\n",
      "data integrity checked OK for 500 sections\n"
     ]
    }
   ],
   "source": [
    "sample_no = 1000\n",
    "#params, segments, Y = t.load_data(from_to=(0,109))\n",
    "# params, segments, Y = t.load_data(path='data/engadiner-5000.json', from_to=(109,198))\n",
    "# params, segments, Y = t.load_data(path='data/LearningData.json', from_to=(109,198))\n",
    "#params, segments, Y = t.load_data(path='data/engadin_5000_mini_100.json', integrity_check = True) params, segments, Y = t.load_data(path='data/engadin_2000.json', \n",
    "#                                   from_to = (0,1000), integrity_check = True)\n",
    "params, segments, Y = t.load_data(path='data/engadin_2000.json', \n",
    "                                  from_to = (0,500), integrity_check = True)\n",
    "#print(params[0], segments[0][0])   \n",
    "#print(type(segments))\n",
    "# params = params[:sample_no]\n",
    "\n",
    "#print(sample_no,param_dim,segment_dim)\n",
    "#print(sections[1]['segments'])\n",
    "#test_sections = sections[:3]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segments [[ 0.         -1.67224553]\n",
      " [ 0.         -0.23282122]\n",
      " [ 0.         -0.21816005]\n",
      " [ 0.         -0.13615355]\n",
      " [ 0.         -0.19578224]\n",
      " [ 0.         -0.27777846]\n",
      " [ 0.         -0.04060345]\n",
      " [ 0.         -0.01748145]\n",
      " [ 0.         -0.09136229]\n",
      " [ 0.          0.17705363]\n",
      " [ 0.          0.05374852]\n",
      " [ 0.          0.03560465]\n",
      " [ 0.          1.69776791]\n",
      " [ 0.         -0.1283162 ]\n",
      " [ 0.         -0.26957736]\n",
      " [ 0.         -0.22415043]\n",
      " [ 0.         -0.1361619 ]\n",
      " [ 0.         -0.10572264]\n",
      " [ 0.         -0.2154851 ]\n",
      " [ 0.2953517  -0.21548574]\n",
      " [ 1.79287486 -0.23879908]\n",
      " [ 5.70162736 -0.10975529]\n",
      " [ 2.95856534 -0.31917724]\n",
      " [ 0.9081236  -0.12832166]\n",
      " [ 0.         -0.3147969 ]\n",
      " [ 0.         -0.24689456]\n",
      " [ 0.         -0.40090442]\n",
      " [ 0.         -0.39675524]\n",
      " [ 0.         -0.40451843]\n",
      " [ 0.         -0.30018003]\n",
      " [ 0.         -0.3730255 ]\n",
      " [ 0.         -0.35978496]\n",
      " [ 0.         -0.37810251]\n",
      " [ 0.         -0.34477096]\n",
      " [ 0.         -0.45985538]\n",
      " [ 0.         -0.41066612]\n",
      " [ 0.         -0.41066677]\n",
      " [ 0.         -0.31480332]\n",
      " [ 0.         -0.3650273 ]\n",
      " [ 0.         -0.19214929]\n",
      " [ 0.         -0.05096614]\n",
      " [ 0.         -0.15565865]\n",
      " [ 0.         -0.09872322]\n",
      " [ 0.         -0.26959181]\n",
      " [ 0.         -1.9279208 ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "params [[-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]\n",
      " [-0.23747519]]\n",
      "Y [ 5.76077854  4.94659706  4.41775553  4.56527519  4.53804683  4.78727157\n",
      "  4.76568706  4.77249195  4.80832012  4.73749057  4.54689589  4.88851759\n",
      "  4.71479272  5.24764375  4.41770884  4.7923605   4.7463748   4.82247119\n",
      "  3.97174791  3.89067947  4.02821105  4.98310704  5.44446179  5.26751372\n",
      "  4.38135774  4.26703054  4.8400654   5.62831899  5.7405942   5.19002951]\n"
     ]
    }
   ],
   "source": [
    "# print(\"segments\", segments[:20,1])\n",
    "print(\"segments\", segments[3])\n",
    "print(\"params\", params[:10])\n",
    "print(\"Y\",Y[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = m.lstm(param_dim, segment_dim, lr=0.005)\n",
    "# 0.1086 oder 0.098 --> sqr = 0.33 --> *7 = 2.0\n",
    "# val loss 4. oder 5. --> sqr = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with early stopping\n",
      "train lstm\n",
      "Epoch 1/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 3.3647 - mean_squared_error: 3.3647\n",
      "Epoch 00001: val_loss improved from inf to 1.08776, storing weights.\n",
      "47/47 [==============================] - 11s 240ms/step - loss: 3.3064 - mean_squared_error: 3.3064 - val_loss: 1.0878 - val_mean_squared_error: 1.0878\n",
      "Epoch 2/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 1.2160 - mean_squared_error: 1.2160\n",
      "Epoch 00002: val_loss is 1.60360, did not improve\n",
      "47/47 [==============================] - 11s 230ms/step - loss: 1.2074 - mean_squared_error: 1.2074 - val_loss: 1.6036 - val_mean_squared_error: 1.6036\n",
      "Epoch 3/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.9867 - mean_squared_error: 0.9867\n",
      "Epoch 00003: val_loss improved from 1.08776 to 0.70094, storing weights.\n",
      "47/47 [==============================] - 11s 242ms/step - loss: 0.9782 - mean_squared_error: 0.9782 - val_loss: 0.7009 - val_mean_squared_error: 0.7009\n",
      "Epoch 4/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8833 - mean_squared_error: 0.8833\n",
      "Epoch 00004: val_loss improved from 0.70094 to 0.68437, storing weights.\n",
      "47/47 [==============================] - 11s 234ms/step - loss: 0.8754 - mean_squared_error: 0.8754 - val_loss: 0.6844 - val_mean_squared_error: 0.6844\n",
      "Epoch 5/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8721 - mean_squared_error: 0.8721\n",
      "Epoch 00005: val_loss improved from 0.68437 to 0.65658, storing weights.\n",
      "47/47 [==============================] - 11s 239ms/step - loss: 0.8666 - mean_squared_error: 0.8666 - val_loss: 0.6566 - val_mean_squared_error: 0.6566\n",
      "Epoch 6/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8046 - mean_squared_error: 0.8046\n",
      "Epoch 00006: val_loss improved from 0.65658 to 0.63837, storing weights.\n",
      "47/47 [==============================] - 11s 231ms/step - loss: 0.8017 - mean_squared_error: 0.8017 - val_loss: 0.6384 - val_mean_squared_error: 0.6384\n",
      "Epoch 7/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8114 - mean_squared_error: 0.8114\n",
      "Epoch 00007: val_loss is 0.63936, did not improve\n",
      "47/47 [==============================] - 11s 241ms/step - loss: 0.8058 - mean_squared_error: 0.8058 - val_loss: 0.6394 - val_mean_squared_error: 0.6394\n",
      "Epoch 8/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8060 - mean_squared_error: 0.8060\n",
      "Epoch 00008: val_loss improved from 0.63837 to 0.62344, storing weights.\n",
      "47/47 [==============================] - 12s 252ms/step - loss: 0.8008 - mean_squared_error: 0.8008 - val_loss: 0.6234 - val_mean_squared_error: 0.6234\n",
      "Epoch 9/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8250 - mean_squared_error: 0.8250\n",
      "Epoch 00009: val_loss is 0.64691, did not improve\n",
      "47/47 [==============================] - 11s 231ms/step - loss: 0.8163 - mean_squared_error: 0.8163 - val_loss: 0.6469 - val_mean_squared_error: 0.6469\n",
      "Epoch 10/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8054 - mean_squared_error: 0.8054\n",
      "Epoch 00010: val_loss is 0.62389, did not improve\n",
      "47/47 [==============================] - 11s 243ms/step - loss: 0.8018 - mean_squared_error: 0.8018 - val_loss: 0.6239 - val_mean_squared_error: 0.6239\n",
      "Epoch 11/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.8159 - mean_squared_error: 0.8159\n",
      "Epoch 00011: val_loss is 0.64474, did not improve\n",
      "47/47 [==============================] - 12s 252ms/step - loss: 0.8112 - mean_squared_error: 0.8112 - val_loss: 0.6447 - val_mean_squared_error: 0.6447\n",
      "Epoch 12/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7798 - mean_squared_error: 0.7798\n",
      "Epoch 00012: val_loss improved from 0.62344 to 0.62163, storing weights.\n",
      "47/47 [==============================] - 11s 235ms/step - loss: 0.7741 - mean_squared_error: 0.7741 - val_loss: 0.6216 - val_mean_squared_error: 0.6216\n",
      "Epoch 13/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7978 - mean_squared_error: 0.7978\n",
      "Epoch 00013: val_loss improved from 0.62163 to 0.60581, storing weights.\n",
      "47/47 [==============================] - 12s 253ms/step - loss: 0.7918 - mean_squared_error: 0.7918 - val_loss: 0.6058 - val_mean_squared_error: 0.6058\n",
      "Epoch 14/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7994 - mean_squared_error: 0.7994\n",
      "Epoch 00014: val_loss is 0.61020, did not improve\n",
      "47/47 [==============================] - 11s 242ms/step - loss: 0.7946 - mean_squared_error: 0.7946 - val_loss: 0.6102 - val_mean_squared_error: 0.6102\n",
      "Epoch 15/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7779 - mean_squared_error: 0.7779\n",
      "Epoch 00015: val_loss is 0.61562, did not improve\n",
      "47/47 [==============================] - 12s 250ms/step - loss: 0.7743 - mean_squared_error: 0.7743 - val_loss: 0.6156 - val_mean_squared_error: 0.6156\n",
      "Epoch 16/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7678 - mean_squared_error: 0.7678\n",
      "Epoch 00016: val_loss is 0.62400, did not improve\n",
      "47/47 [==============================] - 12s 247ms/step - loss: 0.7651 - mean_squared_error: 0.7651 - val_loss: 0.6240 - val_mean_squared_error: 0.6240\n",
      "Epoch 17/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7817 - mean_squared_error: 0.7817\n",
      "Epoch 00017: val_loss is 0.61698, did not improve\n",
      "47/47 [==============================] - 11s 243ms/step - loss: 0.7808 - mean_squared_error: 0.7808 - val_loss: 0.6170 - val_mean_squared_error: 0.6170\n",
      "Epoch 18/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7809 - mean_squared_error: 0.7809\n",
      "Epoch 00018: val_loss is 0.67802, did not improve\n",
      "47/47 [==============================] - 12s 248ms/step - loss: 0.7797 - mean_squared_error: 0.7797 - val_loss: 0.6780 - val_mean_squared_error: 0.6780\n",
      "Epoch 19/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7615 - mean_squared_error: 0.7615\n",
      "Epoch 00019: val_loss is 0.67201, did not improve\n",
      "47/47 [==============================] - 12s 255ms/step - loss: 0.7581 - mean_squared_error: 0.7581 - val_loss: 0.6720 - val_mean_squared_error: 0.6720\n",
      "Epoch 20/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7316 - mean_squared_error: 0.7316\n",
      "Epoch 00020: val_loss is 0.77007, did not improve\n",
      "47/47 [==============================] - 12s 253ms/step - loss: 0.7390 - mean_squared_error: 0.7390 - val_loss: 0.7701 - val_mean_squared_error: 0.7701\n",
      "Epoch 21/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7865 - mean_squared_error: 0.7865\n",
      "Epoch 00021: val_loss is 0.71979, did not improve\n",
      "47/47 [==============================] - 12s 261ms/step - loss: 0.7870 - mean_squared_error: 0.7870 - val_loss: 0.7198 - val_mean_squared_error: 0.7198\n",
      "Epoch 22/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7184 - mean_squared_error: 0.7184\n",
      "Epoch 00022: val_loss is 0.88776, did not improve\n",
      "47/47 [==============================] - 11s 229ms/step - loss: 0.7137 - mean_squared_error: 0.7137 - val_loss: 0.8878 - val_mean_squared_error: 0.8878\n",
      "Epoch 23/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7600 - mean_squared_error: 0.7600\n",
      "Epoch 00023: val_loss is 0.62131, did not improve\n",
      "47/47 [==============================] - 12s 263ms/step - loss: 0.7562 - mean_squared_error: 0.7562 - val_loss: 0.6213 - val_mean_squared_error: 0.6213\n",
      "Epoch 24/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7645 - mean_squared_error: 0.7645\n",
      "Epoch 00024: val_loss is 0.77051, did not improve\n",
      "47/47 [==============================] - 12s 245ms/step - loss: 0.7663 - mean_squared_error: 0.7663 - val_loss: 0.7705 - val_mean_squared_error: 0.7705\n",
      "Epoch 25/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.7344 - mean_squared_error: 0.7344\n",
      "Epoch 00025: val_loss is 0.69419, did not improve\n",
      "47/47 [==============================] - 13s 282ms/step - loss: 0.7314 - mean_squared_error: 0.7314 - val_loss: 0.6942 - val_mean_squared_error: 0.6942\n",
      "Epoch 26/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.6932 - mean_squared_error: 0.6932\n",
      "Epoch 00026: val_loss is 0.76365, did not improve\n",
      "47/47 [==============================] - 12s 256ms/step - loss: 0.6876 - mean_squared_error: 0.6876 - val_loss: 0.7637 - val_mean_squared_error: 0.7637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.6545 - mean_squared_error: 0.6545\n",
      "Epoch 00027: val_loss is 0.77250, did not improve\n",
      "47/47 [==============================] - 11s 224ms/step - loss: 0.6519 - mean_squared_error: 0.6519 - val_loss: 0.7725 - val_mean_squared_error: 0.7725\n",
      "Epoch 28/60\n",
      "46/47 [============================>.] - ETA: 0s - loss: 0.6456 - mean_squared_error: 0.6456\n",
      "Epoch 00028: val_loss is 0.75614, did not improve\n",
      "47/47 [==============================] - 11s 226ms/step - loss: 0.6456 - mean_squared_error: 0.6456 - val_loss: 0.7561 - val_mean_squared_error: 0.7561\n",
      "Epoch 00028: early stopping\n",
      "Using epoch 00013 with val_loss: 0.60581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd23353f7f0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (model, X, Y, split=200, batch_size=20, epochs=3, verbose = 0, earlystop = False):\n",
    "sample_no = Y.shape[0]\n",
    "m.train_lstm(model, (params, segments), Y, split=int(sample_no*0.75), \n",
    "             batch_size=16, epochs=60, verbose = 1, earlystop = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###    Testing models (mlp, lstm, multi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
