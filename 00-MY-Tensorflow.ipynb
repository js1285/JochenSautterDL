{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb as pdb\n",
    "import time\n",
    "from IPython.core.debugger import set_trace\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_1:0\", shape=(), dtype=float32) Tensor(\"Const_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpn4fh09g4\n",
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_master': '', '_num_ps_replicas': 0, '_model_dir': '/tmp/tmpn4fh09g4', '_is_chief': True, '_save_checkpoints_steps': None, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd62af43278>, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_save_checkpoints_secs': 600, '_num_worker_replicas': 1, '_session_config': None, '_service': None, '_task_id': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpn4fh09g4/model.ckpt.\n",
      "INFO:tensorflow:loss = 10.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 515.067\n",
      "INFO:tensorflow:loss = 0.24503, step = 101 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 739.701\n",
      "INFO:tensorflow:loss = 0.0134421, step = 201 (0.136 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 300 into /tmp/tmpn4fh09g4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000386721.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-12-20:31:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpn4fh09g4/model.ckpt-300\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-12-20:31:59\n",
      "INFO:tensorflow:Saving dict for global step 300: average_loss = 0.00721633, global_step = 300, loss = 0.0288653\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-12-20:31:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpn4fh09g4/model.ckpt-300\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-12-20:32:00\n",
      "INFO:tensorflow:Saving dict for global step 300: average_loss = 0.0564868, global_step = 300, loss = 0.225947\n",
      "train metrics: {'loss': 0.028865336, 'average_loss': 0.0072163339, 'global_step': 300}\n",
      "eval metrics: {'loss': 0.22594725, 'average_loss': 0.056486811, 'global_step': 300}\n"
     ]
    }
   ],
   "source": [
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "# Note that input_fn will be called while constructing the TensorFlow graph, not while running the graph. \n",
    "# What it is returning is a representation of the input data as the fundamental unit of TensorFlow computations, a Tensor (or SparseTensor).\n",
    "# num_epochs is max value, actual steps are defined in estimator.train()\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=2, num_epochs=150, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=100, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=200, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set. steps are number of batch calls (not epochs)\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp9zg3y4iy\n",
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_master': '', '_num_ps_replicas': 0, '_model_dir': '/tmp/tmp9zg3y4iy', '_is_chief': True, '_save_checkpoints_steps': None, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5fc303cc0>, '_keep_checkpoint_every_n_hours': 10000, '_tf_random_seed': None, '_save_checkpoints_secs': 600, '_num_worker_replicas': 1, '_session_config': None, '_service': None, '_task_id': 0, '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp9zg3y4iy/model.ckpt.\n",
      "INFO:tensorflow:loss = 56.2281217708, step = 1\n",
      "INFO:tensorflow:global_step/sec: 614.682\n",
      "INFO:tensorflow:loss = 0.110450641029, step = 101 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.68\n",
      "INFO:tensorflow:loss = 0.00939208302401, step = 201 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 583.056\n",
      "INFO:tensorflow:loss = 0.00138642246703, step = 301 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 431.909\n",
      "INFO:tensorflow:loss = 3.33710780472e-05, step = 401 (0.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.153\n",
      "INFO:tensorflow:loss = 1.10591486986e-05, step = 501 (0.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 439.813\n",
      "INFO:tensorflow:loss = 8.83553302532e-07, step = 601 (0.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 453.964\n",
      "INFO:tensorflow:loss = 2.94313486577e-08, step = 701 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 427.358\n",
      "INFO:tensorflow:loss = 3.13769009757e-09, step = 801 (0.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 454.084\n",
      "INFO:tensorflow:loss = 2.53746801046e-10, step = 901 (0.220 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /tmp/tmp9zg3y4iy/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.99308392887e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-12-20:32:05\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9zg3y4iy/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-12-20:32:06\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 3.4808e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-12-20:32:06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp9zg3y4iy/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-12-20:32:07\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101012\n",
      "train metrics: {'loss': 3.4808042e-11, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010101225, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Declare list of features, we only have one real-valued feature\n",
    "def model_fn(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W*features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # EstimatorSpec connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7., 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "mnist Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fd5fc6edeb8>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fd5fc6edef0>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7fd5fc6edf28>)\n"
     ]
    }
   ],
   "source": [
    "# Mnist beginners tutorial\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(\"mnist {}\".format(mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_ Tensor(\"strided_slice:0\", shape=(10,), dtype=float32)\n",
      "0.9066\n"
     ]
    }
   ],
   "source": [
    "# placeholder for input / output variables first dim \"None\" for undefined number of samples, second flattened vectors\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "# variables for weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "# define model\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# define placeholder for one-hot encoded labels, first dim \"None\" for undefined number of samples\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "# define error function (theoretically - but numerically unstable)\n",
    "cross_entropy_old = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# define error function by shortcutting softmax plus cross entropy directly on unnormalised linear data\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "# define training step, using gradient descent and the cross-entropy loss function defined above \n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "# defining session - as default session (so \"sess\" must not called explicity subsequently)\n",
    "sess = tf.InteractiveSession()\n",
    "# actually initialise all gobal variables as set before in graphs\n",
    "tf.global_variables_initializer().run()\n",
    "# train\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "# check for correctness of prediction, compare model output y\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "print(\"y_ {}\".format(y_[0]))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients\n",
    "# The generated values follow a normal distribution with specified mean and standard deviation, \n",
    "# except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# define constant bias\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# Computes a 2-D convolution given 4-D input and filter tensors. returning 4D tensor of same type as x\n",
    "# ? means zero padding Ã¼berall 2, oder ?\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# first convolutional layer with 32 nodes --> woher die 32?? \n",
    "# initialise weights and bias (slightly randomized)\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# define tensor representing input layer: reshape flattened 748 Vector to 4d tensor 28 x 28 image matrixes\n",
    "# and the final dimension corresponding to the number of color channels.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# convolve, same size (padding, stride...), reduce size to 14x14 by maxpooling\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# define second layer concolving same size, max-pooling --> 7x7   64 features ??\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# convolve, same size (padding, stride...), reduce size to 14x14 by maxpooling\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# we add a fully-connected layer with 1024 neurons to allow processing on the entire image.\n",
    "# We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, \n",
    "# and apply a ReLU.\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# implement dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# final fully connected layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "test\n",
      "accu done\n",
      "accu 0.11190000176429749\n",
      "test accuracy 0.1119\n",
      "test\n",
      "accu done\n",
      "accu 0.09059999883174896\n",
      "test accuracy 0.0906\n",
      "test\n",
      "accu done\n",
      "accu 0.0892999991774559\n",
      "test accuracy 0.0893\n",
      "test\n",
      "accu done\n",
      "accu 0.08659999817609787\n",
      "test accuracy 0.0866\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(100):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 50 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print(\"test\")\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
