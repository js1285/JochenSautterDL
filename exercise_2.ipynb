{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# import _pickle as cPickle\n",
    "# import os\n",
    "# import gzip\n",
    "# from IPython.core.debugger import set_trace\n",
    "# import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise: CNN\n",
    "In this exercise you will implement a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "mnist Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f669bdb6e80>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f669bdb6d30>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x7f669bd02dd8>)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(\"mnist {}\".format(mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input variable\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients\n",
    "# The generated values follow a normal distribution with specified mean and standard deviation, \n",
    "# except that values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked.\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# define constant bias\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# Computes a 2-D convolution given 4-D input and filter tensors. returning 4D tensor of same type as x\n",
    "# ? means zero padding Ã¼berall 2, oder ?\n",
    "# X has 4D data format [batch, height, width, color channels]\n",
    "# filter w has shape [filter_height, filter_width, in_channels, out_channels]  (no of filters, first colors...?)\n",
    "# strides = The stride of the sliding window for each dimension of input \n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# first convolutional layer with 16 filters, one color\n",
    "# initialise weights and bias (slightly randomized)\n",
    "# 3 x 3, 1 input (only one color), 16 output channels\n",
    "W_conv1 = weight_variable([3, 3, 1, 16])\n",
    "b_conv1 = bias_variable([16])\n",
    "\n",
    "# define tensor representing input layer: reshape flattened 748 Vector to 4d tensor 28 x 28 image matrices\n",
    "# and the final dimension corresponding to the number of color channels.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# convolve, same size (padding, stride...), reduce size to 14x14 by maxpooling\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# define second layer concolving same size, max-pooling --> 7x7   64 features ??\n",
    "W_conv2 = weight_variable([3, 3, 16, 16])  # ??\n",
    "b_conv2 = bias_variable([16])\n",
    "\n",
    "# convolve, same size (padding, stride...), reduce size to 14x14 by maxpooling\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# we add a fully-connected layer with 128 neurons to allow processing on the entire image.\n",
    "# We reshape the tensor from the pooling layer into a batch of vectors, multiply by a weight matrix, add a bias, \n",
    "# and apply a ReLU.\n",
    "W_fc1 = weight_variable([7 * 7 * 16, 128])\n",
    "b_fc1 = bias_variable([128])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*16])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# implement dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# final fully connected layer\n",
    "W_fc2 = weight_variable([128, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.155\n",
      "step 10, training accuracy 0.765\n",
      "step 20, training accuracy 0.855\n",
      "step 30, training accuracy 0.86\n",
      "step 40, training accuracy 0.92\n",
      "step 50, training accuracy 0.93\n",
      "step 60, training accuracy 0.925\n",
      "step 70, training accuracy 0.945\n",
      "step 80, training accuracy 0.945\n",
      "step 90, training accuracy 0.945\n",
      "step 100, training accuracy 0.95\n",
      "step 110, training accuracy 0.945\n",
      "step 120, training accuracy 0.955\n",
      "step 130, training accuracy 0.965\n",
      "step 140, training accuracy 0.965\n",
      "step 150, training accuracy 0.955\n",
      "step 160, training accuracy 0.95\n",
      "step 170, training accuracy 0.98\n",
      "step 180, training accuracy 0.965\n",
      "step 190, training accuracy 0.955\n",
      "test\n",
      "test accuracy 0.9765\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(200):\n",
    "    batch = mnist.train.next_batch(200)\n",
    "    if i % 10 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print(\"test\")\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
