{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fifth exercise (Chapter 7)Â¶\n",
    "\n",
    "Chapter 7 is about regularization, in this assignment we will implement various forms of regularization for MLP's. \n",
    "\n",
    "* L2 regularization\n",
    "* Dropout\n",
    "* Early stopping\n",
    "\n",
    "In the following code block, all necessary imports are provided as well as the data generators. You can use your MLP implementation from the previous assignment or the reference implementation attached. In this reference implementations, there are some slight differences with the network that was provided for the previous example, most notably: \n",
    "\n",
    "* The cost function now also has an update function integrated\n",
    "* The data format is standardized for all sets (train, test and validation)\n",
    "\n",
    "## Data\n",
    "\n",
    "We will make use of the digits dataset, standard packed in sklearn. It contains a similar classification task as MNIST, however the images are smaller (64 pixels), making experimenting with it faster. \n",
    "\n",
    "For your convenience, a convert function has been provided to convert the dataset into the format that the reference network accepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "import random\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "testsize = len(X) / 10\n",
    "validsize = len(X) / 10\n",
    "\n",
    "train_indices = np.arange(len(X) - testsize - validsize, dtype=np.int)\n",
    "valid_indices = np.arange(len(train_indices), len(X) - testsize, dtype=np.int)\n",
    "test_indices = np.arange(len(train_indices) + len(valid_indices), len(X), dtype=np.int)\n",
    "\n",
    "train_set = [(np.reshape(X[idx], (len(X[idx]), 1)), vectorized_result(y[idx])) for idx in train_indices]\n",
    "valid_set = [(np.reshape(X[idx], (len(X[idx]), 1)), vectorized_result(y[idx])) for idx in valid_indices]\n",
    "test_set  = [(np.reshape(X[idx], (len(X[idx]), 1)), vectorized_result(y[idx])) for idx in test_indices]\n",
    "\n",
    "num_features = len(X[0])\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Network\n",
    "\n",
    "Replace the cell below with your own network from exercise 6. Make sure that it doesn't deviate to much from the reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "class QuadraticCost(object):\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "        \n",
    "        a is an array (size (num_classes, 1)) the activations of the output_layer,\n",
    "        y is an array (same size) with the class label.\n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the derivative of the cost function wrt z.\n",
    "           Recall that a=sigmoid(z).\n",
    "           \n",
    "           z, a and y are arrays of shape (num_classes, 1)\n",
    "        \"\"\"\n",
    "        return (a - y) * sigmoid_prime(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def update(w, nw, eta, size):\n",
    "        \"\"\"Does a single gradient step. \n",
    "        \n",
    "        w and nw are arrays with size corresponding to the weights vector; \n",
    "        eta and size are scalars. \n",
    "        \"\"\"\n",
    "        return w - (eta / size) * nw\n",
    "\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=QuadraticCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.cost = cost\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            validation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs. Here, x represents an array with dimensions \n",
    "        (num_attributes, 1); y is an array of size (num_classes, 1) \n",
    "        The other non-optional parameters are self-explanatory.  \n",
    "        If ``test_data`` is provided (same format as training_data) \n",
    "        then the network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "\n",
    "        if validation_data: n_data = len(validation_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k + mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(validation_data)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(validation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(validation_data), n_data))\n",
    "        return training_cost, training_accuracy, evaluation_cost, evaluation_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)`` (similar to \n",
    "        the data structure defined in SGD) and ``eta`` is the learning \n",
    "        rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [self.cost.update(w, nw, eta, len(mini_batch)) for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [self.cost.update(b, nb, eta, len(mini_batch)) for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # list to store all the activations, layer by layer\n",
    "        zs = []  # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost.delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used in the following way:\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l + 1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.\n",
    "        \"\"\"\n",
    "        results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            cost += self.cost.fn(a, y) / len(data)\n",
    "        return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization\n",
    "\n",
    "Implement L2 regularization using the equations 7.1 - 7.5, by creating a new cost class (containing hyperparameter alpha), containing the same functions as QuadraticCost, i.e., fn, delta and update. Plot how test performance varies for various values of alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n",
      "Cost on training data: 0.9573063868126391\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.9660040168109656\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 1 complete\n",
      "Cost on training data: 0.8963194635880767\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.9058775390700313\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 2 complete\n",
      "Cost on training data: 0.8890584461201172\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.897270959539085\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 3 complete\n",
      "Cost on training data: 0.8856481422829275\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8930196891519119\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 4 complete\n",
      "Cost on training data: 0.882908761825506\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8897345131143851\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 5 complete\n",
      "Cost on training data: 0.8801462594932528\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8867392281347275\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 6 complete\n",
      "Cost on training data: 0.8765373859518206\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8836770086600495\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 7 complete\n",
      "Cost on training data: 0.8722608129329291\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8806174785235902\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 8 complete\n",
      "Cost on training data: 0.868390754843722\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8775796028385888\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 9 complete\n",
      "Cost on training data: 0.8644710859082533\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.874511838594625\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 10 complete\n",
      "Cost on training data: 0.8601990683582305\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8715785284670645\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 11 complete\n",
      "Cost on training data: 0.8546364993745301\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8681947778125814\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 12 complete\n",
      "Cost on training data: 0.8485476784482058\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8646698990466755\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 13 complete\n",
      "Cost on training data: 0.8422490113604292\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8608343586248824\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 14 complete\n",
      "Cost on training data: 0.8349600993195418\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8550177786659509\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 15 complete\n",
      "Cost on training data: 0.8262425458380624\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8480861533919674\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 16 complete\n",
      "Cost on training data: 0.8178098955275583\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8411362187003677\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 17 complete\n",
      "Cost on training data: 0.8108582075343206\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8354956295076346\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 18 complete\n",
      "Cost on training data: 0.8048953259830705\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8308391165947526\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 19 complete\n",
      "Cost on training data: 0.7995018238436018\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8267507924516642\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 20 complete\n",
      "Cost on training data: 0.7943895825488336\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.822511690207469\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 21 complete\n",
      "Cost on training data: 0.7896978756539657\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8193629244002386\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 22 complete\n",
      "Cost on training data: 0.7851459121392107\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8163556700962701\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 23 complete\n",
      "Cost on training data: 0.7808538407000132\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8127724507821157\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 24 complete\n",
      "Cost on training data: 0.7768683530405774\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8088700752946459\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 25 complete\n",
      "Cost on training data: 0.7732315548535768\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8053308150516968\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 26 complete\n",
      "Cost on training data: 0.7698714172848413\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.8023474606377309\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 27 complete\n",
      "Cost on training data: 0.7666866504223604\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7999142187019043\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 28 complete\n",
      "Cost on training data: 0.7636227209249645\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7974557502672506\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 29 complete\n",
      "Cost on training data: 0.7605696141633365\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7948958854773658\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 30 complete\n",
      "Cost on training data: 0.7573414612909739\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7922056604039408\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 31 complete\n",
      "Cost on training data: 0.7539679428718639\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.789886638914619\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 32 complete\n",
      "Cost on training data: 0.7502320455514921\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7868056283707952\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 33 complete\n",
      "Cost on training data: 0.7457268732425724\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7831359885127597\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 34 complete\n",
      "Cost on training data: 0.739137253281516\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7780939715970581\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 35 complete\n",
      "Cost on training data: 0.718395245141996\n",
      "Accuracy on training data: 143 / 1438\n",
      "Cost on evaluation data: 0.7605566584697057\n",
      "Accuracy on evaluation data: 17 / 179\n",
      "Epoch 36 complete\n",
      "Cost on training data: 0.37701070451428803\n",
      "Accuracy on training data: 666 / 1438\n",
      "Cost on evaluation data: 0.42270695866622\n",
      "Accuracy on evaluation data: 66 / 179\n",
      "Epoch 37 complete\n",
      "Cost on training data: 0.3462921669088371\n",
      "Accuracy on training data: 688 / 1438\n",
      "Cost on evaluation data: 0.3866451832032067\n",
      "Accuracy on evaluation data: 70 / 179\n",
      "Epoch 38 complete\n",
      "Cost on training data: 0.3405178466820056\n",
      "Accuracy on training data: 698 / 1438\n",
      "Cost on evaluation data: 0.38047992006470266\n",
      "Accuracy on evaluation data: 71 / 179\n",
      "Epoch 39 complete\n",
      "Cost on training data: 0.33625260667134016\n",
      "Accuracy on training data: 718 / 1438\n",
      "Cost on evaluation data: 0.3762860424680367\n",
      "Accuracy on evaluation data: 72 / 179\n",
      "Epoch 40 complete\n",
      "Cost on training data: 0.3323460400527463\n",
      "Accuracy on training data: 735 / 1438\n",
      "Cost on evaluation data: 0.3723996681316393\n",
      "Accuracy on evaluation data: 76 / 179\n",
      "Epoch 41 complete\n",
      "Cost on training data: 0.3285693816033309\n",
      "Accuracy on training data: 752 / 1438\n",
      "Cost on evaluation data: 0.36896343711459856\n",
      "Accuracy on evaluation data: 76 / 179\n",
      "Epoch 42 complete\n",
      "Cost on training data: 0.32499912508734846\n",
      "Accuracy on training data: 770 / 1438\n",
      "Cost on evaluation data: 0.36549371412972365\n",
      "Accuracy on evaluation data: 78 / 179\n",
      "Epoch 43 complete\n",
      "Cost on training data: 0.321688032059794\n",
      "Accuracy on training data: 782 / 1438\n",
      "Cost on evaluation data: 0.36267507654690395\n",
      "Accuracy on evaluation data: 78 / 179\n",
      "Epoch 44 complete\n",
      "Cost on training data: 0.31836752684069414\n",
      "Accuracy on training data: 792 / 1438\n",
      "Cost on evaluation data: 0.35929598086128506\n",
      "Accuracy on evaluation data: 81 / 179\n",
      "Epoch 45 complete\n",
      "Cost on training data: 0.3150596416229201\n",
      "Accuracy on training data: 798 / 1438\n",
      "Cost on evaluation data: 0.3569794499694215\n",
      "Accuracy on evaluation data: 81 / 179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 complete\n",
      "Cost on training data: 0.3120944114845417\n",
      "Accuracy on training data: 806 / 1438\n",
      "Cost on evaluation data: 0.3539943690121596\n",
      "Accuracy on evaluation data: 84 / 179\n",
      "Epoch 47 complete\n",
      "Cost on training data: 0.30930951608874874\n",
      "Accuracy on training data: 815 / 1438\n",
      "Cost on evaluation data: 0.3514764576820748\n",
      "Accuracy on evaluation data: 85 / 179\n",
      "Epoch 48 complete\n",
      "Cost on training data: 0.3066299006874306\n",
      "Accuracy on training data: 820 / 1438\n",
      "Cost on evaluation data: 0.3482109302485972\n",
      "Accuracy on evaluation data: 87 / 179\n",
      "Epoch 49 complete\n",
      "Cost on training data: 0.303839427956249\n",
      "Accuracy on training data: 831 / 1438\n",
      "Cost on evaluation data: 0.3459958988862232\n",
      "Accuracy on evaluation data: 87 / 179\n",
      "Epoch 50 complete\n",
      "Cost on training data: 0.30127127756726046\n",
      "Accuracy on training data: 840 / 1438\n",
      "Cost on evaluation data: 0.345045089201664\n",
      "Accuracy on evaluation data: 86 / 179\n",
      "Epoch 51 complete\n",
      "Cost on training data: 0.2986623643882936\n",
      "Accuracy on training data: 847 / 1438\n",
      "Cost on evaluation data: 0.34207934829044967\n",
      "Accuracy on evaluation data: 85 / 179\n",
      "Epoch 52 complete\n",
      "Cost on training data: 0.2959548572360201\n",
      "Accuracy on training data: 855 / 1438\n",
      "Cost on evaluation data: 0.3403812415656641\n",
      "Accuracy on evaluation data: 87 / 179\n",
      "Epoch 53 complete\n",
      "Cost on training data: 0.29337398117176994\n",
      "Accuracy on training data: 864 / 1438\n",
      "Cost on evaluation data: 0.33754968498053783\n",
      "Accuracy on evaluation data: 90 / 179\n",
      "Epoch 54 complete\n",
      "Cost on training data: 0.2907268214027991\n",
      "Accuracy on training data: 871 / 1438\n",
      "Cost on evaluation data: 0.33647122208636077\n",
      "Accuracy on evaluation data: 88 / 179\n",
      "Epoch 55 complete\n",
      "Cost on training data: 0.28796917999107785\n",
      "Accuracy on training data: 884 / 1438\n",
      "Cost on evaluation data: 0.3344888251347786\n",
      "Accuracy on evaluation data: 88 / 179\n",
      "Epoch 56 complete\n",
      "Cost on training data: 0.285217900106948\n",
      "Accuracy on training data: 892 / 1438\n",
      "Cost on evaluation data: 0.33292226883014286\n",
      "Accuracy on evaluation data: 89 / 179\n",
      "Epoch 57 complete\n",
      "Cost on training data: 0.2824489876261358\n",
      "Accuracy on training data: 901 / 1438\n",
      "Cost on evaluation data: 0.33135964684058905\n",
      "Accuracy on evaluation data: 91 / 179\n",
      "Epoch 58 complete\n",
      "Cost on training data: 0.2798839471453946\n",
      "Accuracy on training data: 904 / 1438\n",
      "Cost on evaluation data: 0.3288768309040814\n",
      "Accuracy on evaluation data: 92 / 179\n",
      "Epoch 59 complete\n",
      "Cost on training data: 0.27713759991329723\n",
      "Accuracy on training data: 911 / 1438\n",
      "Cost on evaluation data: 0.32736732887598535\n",
      "Accuracy on evaluation data: 94 / 179\n",
      "Epoch 60 complete\n",
      "Cost on training data: 0.2742967318385562\n",
      "Accuracy on training data: 921 / 1438\n",
      "Cost on evaluation data: 0.32571210596564454\n",
      "Accuracy on evaluation data: 93 / 179\n",
      "Epoch 61 complete\n",
      "Cost on training data: 0.27159335767933784\n",
      "Accuracy on training data: 928 / 1438\n",
      "Cost on evaluation data: 0.3232628092985664\n",
      "Accuracy on evaluation data: 94 / 179\n",
      "Epoch 62 complete\n",
      "Cost on training data: 0.2690345414124205\n",
      "Accuracy on training data: 936 / 1438\n",
      "Cost on evaluation data: 0.32120938864747767\n",
      "Accuracy on evaluation data: 95 / 179\n",
      "Epoch 63 complete\n",
      "Cost on training data: 0.2663135808105687\n",
      "Accuracy on training data: 945 / 1438\n",
      "Cost on evaluation data: 0.3186951278605012\n",
      "Accuracy on evaluation data: 97 / 179\n",
      "Epoch 64 complete\n",
      "Cost on training data: 0.2637015230953299\n",
      "Accuracy on training data: 951 / 1438\n",
      "Cost on evaluation data: 0.3175776517357238\n",
      "Accuracy on evaluation data: 97 / 179\n",
      "Epoch 65 complete\n",
      "Cost on training data: 0.2609721310680503\n",
      "Accuracy on training data: 964 / 1438\n",
      "Cost on evaluation data: 0.3154370883843398\n",
      "Accuracy on evaluation data: 98 / 179\n",
      "Epoch 66 complete\n",
      "Cost on training data: 0.25831770128338216\n",
      "Accuracy on training data: 969 / 1438\n",
      "Cost on evaluation data: 0.31479953161455293\n",
      "Accuracy on evaluation data: 97 / 179\n",
      "Epoch 67 complete\n",
      "Cost on training data: 0.25564863545726646\n",
      "Accuracy on training data: 977 / 1438\n",
      "Cost on evaluation data: 0.31142159618670073\n",
      "Accuracy on evaluation data: 98 / 179\n",
      "Epoch 68 complete\n",
      "Cost on training data: 0.2530649818679773\n",
      "Accuracy on training data: 984 / 1438\n",
      "Cost on evaluation data: 0.30873194232455026\n",
      "Accuracy on evaluation data: 100 / 179\n",
      "Epoch 69 complete\n",
      "Cost on training data: 0.2505044923122801\n",
      "Accuracy on training data: 989 / 1438\n",
      "Cost on evaluation data: 0.3071330395365781\n",
      "Accuracy on evaluation data: 98 / 179\n",
      "Epoch 70 complete\n",
      "Cost on training data: 0.24803662752987907\n",
      "Accuracy on training data: 994 / 1438\n",
      "Cost on evaluation data: 0.3052970141003919\n",
      "Accuracy on evaluation data: 101 / 179\n",
      "Epoch 71 complete\n",
      "Cost on training data: 0.24557796367628448\n",
      "Accuracy on training data: 1004 / 1438\n",
      "Cost on evaluation data: 0.3027412695295706\n",
      "Accuracy on evaluation data: 100 / 179\n",
      "Epoch 72 complete\n",
      "Cost on training data: 0.24319155713728602\n",
      "Accuracy on training data: 1005 / 1438\n",
      "Cost on evaluation data: 0.30046975329228287\n",
      "Accuracy on evaluation data: 102 / 179\n",
      "Epoch 73 complete\n",
      "Cost on training data: 0.24071071211641762\n",
      "Accuracy on training data: 1007 / 1438\n",
      "Cost on evaluation data: 0.2989446218340943\n",
      "Accuracy on evaluation data: 101 / 179\n",
      "Epoch 74 complete\n",
      "Cost on training data: 0.23828916878840445\n",
      "Accuracy on training data: 1016 / 1438\n",
      "Cost on evaluation data: 0.2965696363001928\n",
      "Accuracy on evaluation data: 101 / 179\n",
      "Epoch 75 complete\n",
      "Cost on training data: 0.23588691831396535\n",
      "Accuracy on training data: 1020 / 1438\n",
      "Cost on evaluation data: 0.29494388153411044\n",
      "Accuracy on evaluation data: 102 / 179\n",
      "Epoch 76 complete\n",
      "Cost on training data: 0.23349498262156104\n",
      "Accuracy on training data: 1018 / 1438\n",
      "Cost on evaluation data: 0.2924676286970644\n",
      "Accuracy on evaluation data: 104 / 179\n",
      "Epoch 77 complete\n",
      "Cost on training data: 0.23115097179551408\n",
      "Accuracy on training data: 1028 / 1438\n",
      "Cost on evaluation data: 0.2901822916779442\n",
      "Accuracy on evaluation data: 106 / 179\n",
      "Epoch 78 complete\n",
      "Cost on training data: 0.2285170747720464\n",
      "Accuracy on training data: 1029 / 1438\n",
      "Cost on evaluation data: 0.2893528040851036\n",
      "Accuracy on evaluation data: 105 / 179\n",
      "Epoch 79 complete\n",
      "Cost on training data: 0.2262744758370399\n",
      "Accuracy on training data: 1030 / 1438\n",
      "Cost on evaluation data: 0.2876980266811905\n",
      "Accuracy on evaluation data: 106 / 179\n",
      "Epoch 80 complete\n",
      "Cost on training data: 0.22380944010895595\n",
      "Accuracy on training data: 1038 / 1438\n",
      "Cost on evaluation data: 0.284912460576615\n",
      "Accuracy on evaluation data: 106 / 179\n",
      "Epoch 81 complete\n",
      "Cost on training data: 0.22152462293022562\n",
      "Accuracy on training data: 1042 / 1438\n",
      "Cost on evaluation data: 0.2839288847931171\n",
      "Accuracy on evaluation data: 107 / 179\n",
      "Epoch 82 complete\n",
      "Cost on training data: 0.2192504403092654\n",
      "Accuracy on training data: 1044 / 1438\n",
      "Cost on evaluation data: 0.2824904587740988\n",
      "Accuracy on evaluation data: 109 / 179\n",
      "Epoch 83 complete\n",
      "Cost on training data: 0.21703809844388727\n",
      "Accuracy on training data: 1048 / 1438\n",
      "Cost on evaluation data: 0.28056098198319596\n",
      "Accuracy on evaluation data: 110 / 179\n",
      "Epoch 84 complete\n",
      "Cost on training data: 0.21480460958933936\n",
      "Accuracy on training data: 1055 / 1438\n",
      "Cost on evaluation data: 0.27837643349860075\n",
      "Accuracy on evaluation data: 109 / 179\n",
      "Epoch 85 complete\n",
      "Cost on training data: 0.21272859159287053\n",
      "Accuracy on training data: 1059 / 1438\n",
      "Cost on evaluation data: 0.2758936139930128\n",
      "Accuracy on evaluation data: 109 / 179\n",
      "Epoch 86 complete\n",
      "Cost on training data: 0.2106631998049519\n",
      "Accuracy on training data: 1067 / 1438\n",
      "Cost on evaluation data: 0.2747672056472582\n",
      "Accuracy on evaluation data: 112 / 179\n",
      "Epoch 87 complete\n",
      "Cost on training data: 0.2086519555682383\n",
      "Accuracy on training data: 1075 / 1438\n",
      "Cost on evaluation data: 0.2724855440670364\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 88 complete\n",
      "Cost on training data: 0.20670860542806355\n",
      "Accuracy on training data: 1079 / 1438\n",
      "Cost on evaluation data: 0.27091582907546535\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 89 complete\n",
      "Cost on training data: 0.20472992702514262\n",
      "Accuracy on training data: 1084 / 1438\n",
      "Cost on evaluation data: 0.26944412341952295\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 90 complete\n",
      "Cost on training data: 0.2028452665369232\n",
      "Accuracy on training data: 1086 / 1438\n",
      "Cost on evaluation data: 0.2689378676478687\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 91 complete\n",
      "Cost on training data: 0.20097107665307146\n",
      "Accuracy on training data: 1087 / 1438\n",
      "Cost on evaluation data: 0.2679651775274732\n",
      "Accuracy on evaluation data: 114 / 179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92 complete\n",
      "Cost on training data: 0.19913354202571643\n",
      "Accuracy on training data: 1088 / 1438\n",
      "Cost on evaluation data: 0.26579122236002234\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 93 complete\n",
      "Cost on training data: 0.19723037120547135\n",
      "Accuracy on training data: 1093 / 1438\n",
      "Cost on evaluation data: 0.2647305516775382\n",
      "Accuracy on evaluation data: 114 / 179\n",
      "Epoch 94 complete\n",
      "Cost on training data: 0.19548276049010868\n",
      "Accuracy on training data: 1094 / 1438\n",
      "Cost on evaluation data: 0.26439305826665094\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 95 complete\n",
      "Cost on training data: 0.1936883106735706\n",
      "Accuracy on training data: 1098 / 1438\n",
      "Cost on evaluation data: 0.2627610047581446\n",
      "Accuracy on evaluation data: 113 / 179\n",
      "Epoch 96 complete\n",
      "Cost on training data: 0.19198326757480097\n",
      "Accuracy on training data: 1101 / 1438\n",
      "Cost on evaluation data: 0.2622137936889636\n",
      "Accuracy on evaluation data: 114 / 179\n",
      "Epoch 97 complete\n",
      "Cost on training data: 0.19026391757194142\n",
      "Accuracy on training data: 1099 / 1438\n",
      "Cost on evaluation data: 0.26058810109338\n",
      "Accuracy on evaluation data: 115 / 179\n",
      "Epoch 98 complete\n",
      "Cost on training data: 0.18864735069362484\n",
      "Accuracy on training data: 1104 / 1438\n",
      "Cost on evaluation data: 0.2588602741360162\n",
      "Accuracy on evaluation data: 114 / 179\n",
      "Epoch 99 complete\n",
      "Cost on training data: 0.18706981660543293\n",
      "Accuracy on training data: 1108 / 1438\n",
      "Cost on evaluation data: 0.2590788304577327\n",
      "Accuracy on evaluation data: 115 / 179\n"
     ]
    }
   ],
   "source": [
    "layers = [num_features, 50, 50, 50, num_classes]\n",
    "net = Network(layers)\n",
    "t_cost, t_accuracy, e_cost, e_accuracy = net.SGD(train_set, 100, 50, 0.1, validation_data=test_set,\n",
    "            monitor_evaluation_cost=True,\n",
    "            monitor_evaluation_accuracy=True,\n",
    "            monitor_training_cost=True,\n",
    "            monitor_training_accuracy=True)   # orig epochs 300                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Implement dropout as explained in 7.12. Create a function generateMask(p_input, p_hidden) that generates a list of l arrays, each element in l being an array of size (layer_size, 1). Each element of the array on the first level is 1 with probability p_input. Each element of the array on the last level (output layer) is 1. Each element of the other arrays is 1 with probability p_hidden. For this you can use the distribution scipy.stats.bernoulli. Add hyperparameters p_input and p_hidden to the network. \n",
    "\n",
    "Inference occurs as presented in equation 7.52. Plot how the test performance varies for various values of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [num_features, 50, 50, 50, num_classes]\n",
    "net = Network(layers)\n",
    "t_cost, t_accuracy, e_cost, e_accuracy = net.SGD(train_set, 300, 50, 0.1, validation_data=test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Implement early stopping as presented in algorithm 7.1 and 7.2. Make sure to make proper use of the validation and the test set (as provided above). \n",
    "\n",
    "Demonstrate the correct usage by plotting learning curves for train, validation and test performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
